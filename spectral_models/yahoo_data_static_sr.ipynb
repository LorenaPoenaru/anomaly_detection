{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sr'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 65\n",
    "test_window = 65\n",
    "data_in_memory_size = 3000\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = 'micro'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e6d0e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\oxifl\\documents\\uni\\anomaly_detection\\utils\\sr\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Cython>=0.29.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.2) (0.29.14)\n",
      "Requirement already satisfied: numpy==1.18.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.2) (1.18.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.2) (0.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.2) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.2) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==0.25.3->msanomalydetector==1.2) (1.15.0)\n",
      "Building wheels for collected packages: msanomalydetector\n",
      "  Building wheel for msanomalydetector (setup.py): started\n",
      "  Building wheel for msanomalydetector (setup.py): finished with status 'done'\n",
      "  Created wheel for msanomalydetector: filename=msanomalydetector-1.2-cp37-cp37m-win_amd64.whl size=85901 sha256=6ac360b45f03e900fd8120a76a40d8a4146c992707ec7ea27c01ef7072bf5551\n",
      "  Stored in directory: C:\\Users\\oxifl\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-tx9o1gm5\\wheels\\55\\14\\b0\\81eaf4ba37022705843ac4c1ee41c3a2d69ded4bc0815ed2e6\n",
      "Successfully built msanomalydetector\n",
      "Installing collected packages: msanomalydetector\n",
      "  Attempting uninstall: msanomalydetector\n",
      "    Found existing installation: msanomalydetector 1.2\n",
      "    Uninstalling msanomalydetector-1.2:\n",
      "      Successfully uninstalled msanomalydetector-1.2\n",
      "Successfully installed msanomalydetector-1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# import SR as a module from ../utils/sr/\n",
    "import sys\n",
    "!{sys.executable} -m pip install ../utils/sr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3373d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f875235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of a general dataset path\n",
    "path_files_yahoo = '../datasets/Yahoo_A1Benchmark/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with current time series: real_1 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_1 for 0.060999393463134766\n",
      "F1 score is: [0.99788285 0.        ]\n",
      "Working with current time series: real_10 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_10 for 0.12199902534484863\n",
      "F1 score is: [0.98945889 0.        ]\n",
      "Working with current time series: real_11 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_11 for 0.04000043869018555\n",
      "F1 score is: [0.98728814 0.18181818]\n",
      "Working with current time series: real_12 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_12 for 0.03799867630004883\n",
      "F1 score is: [0.99790649 0.4       ]\n",
      "Working with current time series: real_13 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_13 for 0.042002201080322266\n",
      "F1 score is: [0.9943899  0.33333333]\n",
      "Working with current time series: real_14 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_14 for 0.040026187896728516\n",
      "F1 score is: [0.99440559 0.        ]\n",
      "Working with current time series: real_15 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_15 for 0.037999629974365234\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16116/4038046825.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     96\u001b[0m             \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n\u001b[1;32m---> 98\u001b[1;33m                                                         y_pred_total_entropy[:data_test.shape[0]][i:i+test_window])\n\u001b[0m\u001b[0;32m     99\u001b[0m             \u001b[0mbatch_metrices_f1_entropy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmet_total\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     MCM = multilabel_confusion_matrix(y_true, y_pred,\n\u001b[0;32m   1489\u001b[0m                                       \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1490\u001b[1;33m                                       labels=labels, samplewise=samplewise)\n\u001b[0m\u001b[0;32m   1491\u001b[0m     \u001b[0mtp_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMCM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1492\u001b[0m     \u001b[0mpred_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtp_sum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mMCM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mmultilabel_confusion_matrix\u001b[1;34m(y_true, y_pred, sample_weight, labels, samplewise)\u001b[0m\n\u001b[0;32m    447\u001b[0m         \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m         \u001b[0msorted_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 273\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode\u001b[1;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         return _encode_numpy(values, uniques, encode,\n\u001b[1;32m--> 118\u001b[1;33m                              check_unknown=check_unknown)\n\u001b[0m\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode_numpy\u001b[1;34m(values, uniques, encode, check_unknown)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_unknown\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_encode_check_unknown\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 raise ValueError(\"y contains previously unseen labels: %s\"\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_label.py\u001b[0m in \u001b[0;36m_encode_check_unknown\u001b[1;34m(values, uniques, return_mask)\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdiff\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m         \u001b[0munique_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muniques\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_mask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0maux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 'kpi', 'NAB_realAWSCloudwatch'\n",
    "for dataset in ['Yahoo_A1Benchmark']:\n",
    "\n",
    "    train_data_path = f'../datasets/{dataset}/'\n",
    "    \n",
    "    for filename in os.listdir(train_data_path):\n",
    "        f = os.path.join(train_data_path, filename)\n",
    "        data = pd.read_csv(f)\n",
    "\n",
    "        filename = filename.replace('.csv', '')\n",
    "        print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "        data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "        data.drop_duplicates(subset=['timestamp'], keep=False, inplace=True)\n",
    "\n",
    "        # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "        if dataset == 'kpi':\n",
    "            data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename))\n",
    "            data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            data['timestamp'] = data['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "            data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "        # 50-50 train/test split\n",
    "        data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "        # train model #######################################################################################\n",
    "        start = time.time()\n",
    "        \n",
    "        model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                                 threshold=sr_model_params[0], mag_window=sr_model_params[1],\n",
    "                                 score_window=sr_model_params[2], sensitivity=sr_model_params[3],\n",
    "                                 detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                                 filename=filename, drift_detector=drift_detector,\n",
    "                                 data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "\n",
    "        model.fit()\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        diff = end_time - start\n",
    "        print(f\"Trained SR on {filename} for {diff}\")\n",
    "\n",
    "        # test model #########################################################################################\n",
    "        batch_metrices_f1_entropy = []\n",
    "        batch_metrices_f1_no_entropy = []\n",
    "        y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "        batches_with_anomalies = []\n",
    "        idx = 0\n",
    "\n",
    "        pred_time = []\n",
    "\n",
    "        if for_optimization:\n",
    "            data_in_memory = pd.DataFrame([])\n",
    "        else:\n",
    "            data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "        for start in range(0, data_test.shape[0], anomaly_window):\n",
    "            try:\n",
    "                # current window on which TESTING and SCORING is applied\n",
    "                window = data_test.iloc[start:start + anomaly_window]\n",
    "                # data hold in memory, calculations and predictions are performed across this window\n",
    "                data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                X, y = window['value'], window['is_anomaly']\n",
    "                if y.tolist():\n",
    "                    if model_name == 'sr':\n",
    "                        model.__series__ = data_in_memory\n",
    "                        try:\n",
    "                            res = model.predict(data_in_memory, window.shape[0])\n",
    "                            y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "\n",
    "                            y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                        except Exception as e:\n",
    "                            y_pred_noe = [0 for _ in range(window.shape[0])]\n",
    "                            y_pred_e = [0 for _ in range(window.shape[0])]\n",
    "\n",
    "                    idx += 1\n",
    "                    y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                    y_pred_total_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_e)][:window.shape[0]]\n",
    "                    y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "                    y_pred_e = y_pred_e[:window.shape[0]]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An awful exception has occured during testing: \\n{repr(e)}\")\n",
    "\n",
    "        # evaluate TS ########################################################################################\n",
    "\n",
    "        # calculate batched metrics per *test_window*\n",
    "        # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "        # this works because we have binary classification\n",
    "        data_reset = data_test.reset_index()['is_anomaly']\n",
    "        for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "            # here, met_total will be (precision, recall, f1_score, support)\n",
    "            # \n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "            batch_metrices_f1_entropy.append(met_total[2][-1])\n",
    "\n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "            batch_metrices_f1_no_entropy.append(met_total[2][-1])\n",
    "\n",
    "        met_total_no_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]])\n",
    "        met_total_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                    y_pred_total_entropy[:data_test.shape[0]])\n",
    "\n",
    "        # add entry to stats #######################################################################################\n",
    "\n",
    "        try:\n",
    "            stats_full = pd.read_csv(f'../results/scores/sr_{dataset}_stats.csv')\n",
    "        except:\n",
    "            stats_full = pd.DataFrame([])\n",
    "\n",
    "        stats_full = stats_full.append({\n",
    "            'model': model_name,\n",
    "            'dataset': filename,\n",
    "            'window': anomaly_window,\n",
    "            'f1-entropy': met_total_entropy[2][-1],\n",
    "            'f1-no_entropy': met_total_no_entropy[2][-1],\n",
    "        }, ignore_index=True)\n",
    "        stats_full.to_csv(f'../results/scores/sr_{dataset}_stats.csv', index=False)\n",
    "        print(f'F1 score is: {met_total_no_entropy[2][-1]}')\n",
    "\n",
    "        # plotting ##################################################################################################\n",
    "        # general on ts\n",
    "        if use_entropy:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_entropy, dataset, filename, model_name)\n",
    "        else:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_no_entropy, dataset, filename, model_name)\n",
    "\n",
    "        # model-specific rendering of internal workings\n",
    "        # use y_pred_total_e (for entropy threshold) or\n",
    "        # y_pred_total_noe for non entropy threshold\n",
    "        # model.plot(data_test, threshold_type=threshold_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
