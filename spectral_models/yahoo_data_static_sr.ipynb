{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "from alibi_detect.od import SpectralResidual as SpectralResidualAlibi\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sr_alibi'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 288\n",
    "test_window = 288\n",
    "data_in_memory_size = 3000\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = 'macro'\n",
    "\n",
    "# allowed delay for anomaly shifts during evaluation\n",
    "evaluation_delay = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e6d0e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\oxifl\\documents\\uni\\anomaly_detection\\utils\\sr\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Cython>=0.29.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (0.29.14)\n",
      "Requirement already satisfied: numpy==1.18.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (1.18.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (0.25.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.3) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.3) (2019.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==0.25.3->msanomalydetector==1.3) (1.15.0)\n",
      "Building wheels for collected packages: msanomalydetector\n",
      "  Building wheel for msanomalydetector (setup.py): started\n",
      "  Building wheel for msanomalydetector (setup.py): finished with status 'done'\n",
      "  Created wheel for msanomalydetector: filename=msanomalydetector-1.3-cp37-cp37m-win_amd64.whl size=85870 sha256=4f8513bedf14562334957d83ea6b168d169df823c9928d92aa2c2ab9129a7966\n",
      "  Stored in directory: C:\\Users\\oxifl\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-gwocg58v\\wheels\\55\\14\\b0\\81eaf4ba37022705843ac4c1ee41c3a2d69ded4bc0815ed2e6\n",
      "Successfully built msanomalydetector\n",
      "Installing collected packages: msanomalydetector\n",
      "  Attempting uninstall: msanomalydetector\n",
      "    Found existing installation: msanomalydetector 1.3\n",
      "    Uninstalling msanomalydetector-1.3:\n",
      "      Successfully uninstalled msanomalydetector-1.3\n",
      "Successfully installed msanomalydetector-1.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# import SR as a module from ../utils/sr/\n",
    "import sys\n",
    "!{sys.executable} -m pip install ../utils/sr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98cbcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(2, '../utils/')\n",
    "\n",
    "from evaluation import label_evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "94e2d44b",
   "metadata": {},
   "source": [
    "# Create SR model from existing alibi detect library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f8d92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No threshold level set. Need to infer threshold using `infer_threshold`.\n"
     ]
    }
   ],
   "source": [
    "od = SpectralResidualAlibi(\n",
    "    threshold=None,                  # threshold for outlier score\n",
    "    window_amp=20,                   # window for the average log amplitude\n",
    "    window_local=20,                 # window for the average saliency map\n",
    "    n_est_points=20                 # nb of estimated points padded to the end of the sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3373d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No threshold level set. Need to infer threshold using `infer_threshold`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with current time series: real_1 in dataset Yahoo_A1Benchmark\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10136/4131011243.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     53\u001b[0m                                             \u001b[0mn_est_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m                 \u001b[1;31m# nb of estimated points padded to the end of the sequence\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                                         )\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfer_threshold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'timestamp'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_perc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\alibi_detect\\od\\sr.py\u001b[0m in \u001b[0;36minfer_threshold\u001b[1;34m(self, X, t, threshold_perc)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# compute outlier scores\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0miscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\alibi_detect\\od\\sr.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, t)\u001b[0m\n\u001b[0;32m    169\u001b[0m                                  'of time series equals {}.'.format(n_dim))\n\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[0mX_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_est_points\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# add padding\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m         \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaliency_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_pad\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# compute saliency map\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0msr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_est_points\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# remove padding again\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\alibi_detect\\od\\sr.py\u001b[0m in \u001b[0;36madd_est_points\u001b[1;34m(self, X, t)\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mPadded\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mof\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \"\"\"\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[0mX_add\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_grad_points\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mX_pad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_add\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_est_points\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\alibi_detect\\od\\sr.py\u001b[0m in \u001b[0;36mcompute_grads\u001b[1;34m(self, X, t)\u001b[0m\n\u001b[0;32m    119\u001b[0m         \"\"\"\n\u001b[0;32m    120\u001b[0m         \u001b[0mdX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_grad_points\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mdt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_grad_points\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m         \u001b[0mmean_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdX\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmean_grads\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1071\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1072\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1073\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, series, key)\u001b[0m\n\u001b[0;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"getitem\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4729\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4730\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"tz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4731\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4732\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# 'Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch', 'kpi'\n",
    "for dataset in ['Yahoo_A1Benchmark']:\n",
    "\n",
    "    if dataset == 'kpi':\n",
    "        train_data_path = f'../datasets/kpi/train/'\n",
    "    else:\n",
    "        train_data_path = f'../datasets/{dataset}/'\n",
    "    \n",
    "    for filename in os.listdir(train_data_path):\n",
    "        f = os.path.join(train_data_path, filename)\n",
    "        data = pd.read_csv(f, engine='python')\n",
    "\n",
    "        filename = filename.replace('.csv', '')\n",
    "        print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "        data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "        data.drop_duplicates(subset=['timestamp'], keep=False, inplace=True)\n",
    "\n",
    "        # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "        if dataset == 'kpi':\n",
    "            data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename + '.csv'))\n",
    "            data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            data['timestamp'] = data['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "            data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "        # 50-50 train/test split\n",
    "        data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "        # train model #######################################################################################\n",
    "        start = time.time()\n",
    "\n",
    "        if model_name == 'sr':\n",
    "        \n",
    "            model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                                    threshold=sr_model_params[0], mag_window=sr_model_params[1],\n",
    "                                    score_window=sr_model_params[2], sensitivity=sr_model_params[3],\n",
    "                                    detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                                    filename=filename, drift_detector=drift_detector,\n",
    "                                    data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "\n",
    "            model.fit()\n",
    "\n",
    "        elif model_name == 'sr_alibi':\n",
    "            data_train_ = data_train[['value']].to_numpy().reshape(-1, 1).astype(np.float32)\n",
    "            model = SpectralResidualAlibi(\n",
    "                                            threshold=None,                  # threshold for outlier score\n",
    "                                            window_amp=20,                   # window for the average log amplitude\n",
    "                                            window_local=20,                 # window for the average saliency map\n",
    "                                            n_est_points=20                 # nb of estimated points padded to the end of the sequence\n",
    "                                        )\n",
    "            model.infer_threshold(data_train_, data_train['timestamp'], threshold_perc=90)\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        diff = end_time - start\n",
    "        print(f\"Trained SR on {filename} for {diff}\")\n",
    "\n",
    "        # test model #########################################################################################\n",
    "\n",
    "        batch_metrices_f1_entropy = []\n",
    "        batch_metrices_f1_no_entropy = []\n",
    "        y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "        batches_with_anomalies = []\n",
    "        idx = 0\n",
    "\n",
    "        pred_time = []\n",
    "\n",
    "        if for_optimization:\n",
    "            data_in_memory = pd.DataFrame([])\n",
    "        else:\n",
    "            data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "        for start in range(0, data_test.shape[0], anomaly_window):\n",
    "            try:\n",
    "                # current window on which TESTING and SCORING is applied\n",
    "                window = data_test.iloc[start:start + anomaly_window]\n",
    "                # data hold in memory, calculations and predictions are performed across this window\n",
    "                data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                X, y = window['value'], window['is_anomaly']\n",
    "                if y.tolist():\n",
    "                    if model_name == 'sr':\n",
    "                        model.__series__ = data_in_memory\n",
    "                        try:\n",
    "                            res = model.predict(data_in_memory, window.shape[0])\n",
    "                            y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "                            y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                        except Exception as e:\n",
    "                            print(str(e))\n",
    "                            y_pred_noe = [0 for _ in range(window.shape[0])]\n",
    "                            y_pred_e = [0 for _ in range(window.shape[0])]\n",
    "                    elif model_name == 'sr_alibi':\n",
    "                        od_preds = model.predict(data_in_memory[['value']].to_numpy().reshape(-1, 1).astype(np.float32), \n",
    "                        data_in_memory['timestamp'], return_instance_score=True)\n",
    "                        y_pred_noe = od_preds['data']['is_outlier'][-anomaly_window:]\n",
    "                        y_pred_e = od_preds['data']['is_outlier'][-anomaly_window:]\n",
    "\n",
    "                    idx += 1\n",
    "                    y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                    y_pred_total_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_e)][:window.shape[0]]\n",
    "                    y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "                    y_pred_e = y_pred_e[:window.shape[0]]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An awful exception has occured during testing: \\n{repr(e)}\")\n",
    "\n",
    "        # evaluate TS ########################################################################################\n",
    "\n",
    "        # calculate batched metrics per *test_window*\n",
    "        # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "        # this works because we have binary classification\n",
    "        data_reset = data_test.reset_index()['is_anomaly']\n",
    "        for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "            # here, met_total will be (precision, recall, f1_score, support)\n",
    "            # \n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_entropy[:data_test.shape[0]][i:i+test_window],\n",
    "                                                        average=avg_type)\n",
    "            batch_metrices_f1_entropy.append(met_total[2])\n",
    "\n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window],\n",
    "                                                        average=avg_type)\n",
    "            batch_metrices_f1_no_entropy.append(met_total[2])\n",
    "\n",
    "        met_total_no_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]],\n",
    "                                                        average=avg_type)\n",
    "        met_total_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                    y_pred_total_entropy[:data_test.shape[0]],\n",
    "                                                    average=avg_type)\n",
    "\n",
    "        # add entry to stats #######################################################################################\n",
    "\n",
    "        try:\n",
    "            stats_full = pd.read_csv(f'../results/scores/sr_{dataset}_stats.csv')\n",
    "        except:\n",
    "            stats_full = pd.DataFrame([])\n",
    "\n",
    "        predicted_df = copy.deepcopy(data_test)\n",
    "        if use_entropy:\n",
    "            predicted_df['is_anomaly'] = y_pred_total_entropy\n",
    "        else:\n",
    "            predicted_df['is_anomaly'] = y_pred_total_no_entropy\n",
    "            \n",
    "        # smoothed_score = label_evaluation(data_test, predicted_df, evaluation_delay)\n",
    "        # smoothed_score_entropy = label_evaluation(data_test, predicted_df, evaluation_delay)\n",
    "        # print(f'Smoothed F1 score is: {smoothed_score}')\n",
    "\n",
    "        stats_full = stats_full.append({\n",
    "            'model': model_name,\n",
    "            'ts_name': filename,\n",
    "            'window': anomaly_window,\n",
    "            'delay': evaluation_delay,\n",
    "            'f1_score': met_total_no_entropy[2],\n",
    "            'labels_true': data_test['is_anomaly'].tolist(),\n",
    "            'labels_pred': y_pred_total_no_entropy[:data_test.shape[0]]\n",
    "\n",
    "        }, ignore_index=True)\n",
    "        stats_full.to_csv(f'../results/scores/sr_{dataset}_stats.csv', index=False)\n",
    "        print(f'My old F1 score is: {met_total_no_entropy[2]}')\n",
    "\n",
    "        # plotting ##################################################################################################\n",
    "        # general on ts\n",
    "        if use_entropy:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_entropy, dataset, filename, model_name)\n",
    "        else:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_no_entropy, dataset, filename, model_name)\n",
    "\n",
    "        # model-specific rendering of internal workings\n",
    "        # use y_pred_total_e (for entropy threshold) or\n",
    "        # y_pred_total_noe for non entropy threshold\n",
    "        # fig = model.plot(data_test, threshold_type=threshold_type)\n",
    "        # fig.write_image(\n",
    "        #     f'../results/imgs/{model_name}_{dataset}_{filename}_saliency_map.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
