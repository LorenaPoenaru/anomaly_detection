{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sr'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 65\n",
    "test_window = 65\n",
    "data_in_memory_size = 3000\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = 'micro'\n",
    "\n",
    "# allowed delay for anomaly shifts during evaluation\n",
    "evaluation_delay = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4e6d0e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Processing c:\\users\\oxifl\\documents\\uni\\anomaly_detection\\utils\\sr\n",
      "  Preparing metadata (setup.py): started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.0 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: Cython>=0.29.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (0.29.14)\n",
      "Requirement already satisfied: numpy==1.18.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (1.18.1)\n",
      "Requirement already satisfied: pandas==0.25.3 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from msanomalydetector==1.3) (0.25.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.3) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas==0.25.3->msanomalydetector==1.3) (2.8.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from python-dateutil>=2.6.1->pandas==0.25.3->msanomalydetector==1.3) (1.15.0)\n",
      "Building wheels for collected packages: msanomalydetector\n",
      "  Building wheel for msanomalydetector (setup.py): started\n",
      "  Building wheel for msanomalydetector (setup.py): finished with status 'done'\n",
      "  Created wheel for msanomalydetector: filename=msanomalydetector-1.3-cp37-cp37m-win_amd64.whl size=85870 sha256=15ee39513548073973d96c79ac2dfcdfdb50b983f2242f16fb86e8ac1afb6a30\n",
      "  Stored in directory: C:\\Users\\oxifl\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-zqrdq6nn\\wheels\\55\\14\\b0\\81eaf4ba37022705843ac4c1ee41c3a2d69ded4bc0815ed2e6\n",
      "Successfully built msanomalydetector\n",
      "Installing collected packages: msanomalydetector\n",
      "  Attempting uninstall: msanomalydetector\n",
      "    Found existing installation: msanomalydetector 1.3\n",
      "    Uninstalling msanomalydetector-1.3:\n",
      "      Successfully uninstalled msanomalydetector-1.3\n",
      "Successfully installed msanomalydetector-1.3\n"
     ]
    }
   ],
   "source": [
    "# import SR as a module from ../utils/sr/\n",
    "import sys\n",
    "!{sys.executable} -m pip install ../utils/sr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "98cbcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(2, '../utils/')\n",
    "\n",
    "from evaluation import label_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3373d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with current time series: real_1 in dataset Yahoo_A1Benchmark\n",
      "Trained SR on real_1 for 0.04303264617919922\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results\\\\imgs\\\\sr_Yahoo_A1Benchmark_real_1_saliency_map.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12040/2027334800.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m# use y_pred_total_e (for entropy threshold) or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;31m# y_pred_total_noe for non entropy threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreshold_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         fig.write_image(\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\msanomalydetector\\spectral_residual.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, datatest, threshold_type)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshowlegend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Saliency map'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__anomaly_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\basedatatypes.py\u001b[0m in \u001b[0;36mwrite_image\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3819\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3821\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3823\u001b[0m     \u001b[1;31m# Static helpers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\plotly\\io\\_kaleido.py\u001b[0m in \u001b[0;36mwrite_image\u001b[1;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;31m# We previously succeeded in interpreting `file` as a pathlib object.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;31m# Now we can use `write_bytes()`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m         \u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\pathlib.py\u001b[0m in \u001b[0;36mwrite_bytes\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1185\u001b[0m         \u001b[1;31m# type-check for the buffer interface before truncating the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1187\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1188\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\pathlib.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         return io.open(self, mode, buffering, encoding, errors, newline,\n\u001b[1;32m-> 1165\u001b[1;33m                        opener=self._opener)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\pathlib.py\u001b[0m in \u001b[0;36m_opener\u001b[1;34m(self, name, flags, mode)\u001b[0m\n\u001b[0;32m   1017\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o666\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m         \u001b[1;31m# A stub for the opener argument to built-in open()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1019\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1020\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0o777\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results\\\\imgs\\\\sr_Yahoo_A1Benchmark_real_1_saliency_map.png'"
     ]
    }
   ],
   "source": [
    "# 'Yahoo_A1Benchmark',\n",
    "for dataset in ['Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch', 'kpi']:\n",
    "\n",
    "    train_data_path = f'../datasets/{dataset}/'\n",
    "    \n",
    "    for filename in os.listdir(train_data_path):\n",
    "        f = os.path.join(train_data_path, filename)\n",
    "        data = pd.read_csv(f)\n",
    "\n",
    "        filename = filename.replace('.csv', '')\n",
    "        print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "        data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "        data.drop_duplicates(subset=['timestamp'], keep=False, inplace=True)\n",
    "\n",
    "        # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "        if dataset == 'kpi':\n",
    "            data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename))\n",
    "            data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            data['timestamp'] = data['timestamp'].apply(\n",
    "                lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "            data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "        # 50-50 train/test split\n",
    "        data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "        # train model #######################################################################################\n",
    "        start = time.time()\n",
    "        \n",
    "        model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                                 threshold=sr_model_params[0], mag_window=sr_model_params[1],\n",
    "                                 score_window=sr_model_params[2], sensitivity=sr_model_params[3],\n",
    "                                 detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                                 filename=filename, drift_detector=drift_detector,\n",
    "                                 data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "\n",
    "        model.fit()\n",
    "        \n",
    "        end_time = time.time()\n",
    "\n",
    "        diff = end_time - start\n",
    "        print(f\"Trained SR on {filename} for {diff}\")\n",
    "\n",
    "        # test model #########################################################################################\n",
    "        batch_metrices_f1_entropy = []\n",
    "        batch_metrices_f1_no_entropy = []\n",
    "        y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "        batches_with_anomalies = []\n",
    "        idx = 0\n",
    "\n",
    "        pred_time = []\n",
    "\n",
    "        if for_optimization:\n",
    "            data_in_memory = pd.DataFrame([])\n",
    "        else:\n",
    "            data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "        for start in range(0, data_test.shape[0], anomaly_window):\n",
    "            try:\n",
    "                # current window on which TESTING and SCORING is applied\n",
    "                window = data_test.iloc[start:start + anomaly_window]\n",
    "                # data hold in memory, calculations and predictions are performed across this window\n",
    "                data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                X, y = window['value'], window['is_anomaly']\n",
    "                if y.tolist():\n",
    "                    if model_name == 'sr':\n",
    "                        model.__series__ = data_in_memory\n",
    "                        try:\n",
    "                            res = model.predict(data_in_memory, window.shape[0])\n",
    "                            y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "                            y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                        except Exception as e:\n",
    "                            print(str(e))\n",
    "                            y_pred_noe = [0 for _ in range(window.shape[0])]\n",
    "                            y_pred_e = [0 for _ in range(window.shape[0])]\n",
    "\n",
    "                    idx += 1\n",
    "                    y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                    y_pred_total_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_e)][:window.shape[0]]\n",
    "                    y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "                    y_pred_e = y_pred_e[:window.shape[0]]\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"An awful exception has occured during testing: \\n{repr(e)}\")\n",
    "\n",
    "        # evaluate TS ########################################################################################\n",
    "\n",
    "        # calculate batched metrics per *test_window*\n",
    "        # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "        # this works because we have binary classification\n",
    "        data_reset = data_test.reset_index()['is_anomaly']\n",
    "        for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "            # here, met_total will be (precision, recall, f1_score, support)\n",
    "            # \n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "            batch_metrices_f1_entropy.append(met_total[2][-1])\n",
    "\n",
    "            met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "            batch_metrices_f1_no_entropy.append(met_total[2][-1])\n",
    "\n",
    "        met_total_no_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                        y_pred_total_no_entropy[:data_test.shape[0]])\n",
    "        met_total_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                    y_pred_total_entropy[:data_test.shape[0]])\n",
    "\n",
    "        # add entry to stats #######################################################################################\n",
    "\n",
    "        try:\n",
    "            stats_full = pd.read_csv(f'../results/scores/sr_{dataset}_stats.csv')\n",
    "        except:\n",
    "            stats_full = pd.DataFrame([])\n",
    "\n",
    "        predicted_df = copy.deepcopy(data_test)\n",
    "        if use_entropy:\n",
    "            predicted_df['is_anomaly'] = y_pred_total_entropy\n",
    "        else:\n",
    "            predicted_df['is_anomaly'] = y_pred_total_no_entropy\n",
    "            \n",
    "        smoothed_score = label_evaluation(data_test, predicted_df, evaluation_delay)\n",
    "        smoothed_score_entropy = label_evaluation(data_test, predicted_df, evaluation_delay)\n",
    "        print(f'Smoothed F1 score is: {smoothed_score}')\n",
    "\n",
    "        stats_full = stats_full.append({\n",
    "            'model': model_name,\n",
    "            'dataset': filename,\n",
    "            'window': anomaly_window,\n",
    "            'f1-entropy': smoothed_score_entropy,\n",
    "            'f1-no_entropy': smoothed_score,\n",
    "            'delay': evaluation_delay,\n",
    "            'f1_score_old_technique': met_total_no_entropy[2][-1]\n",
    "        }, ignore_index=True)\n",
    "        stats_full.to_csv(f'../results/scores/sr_{dataset}_stats.csv', index=False)\n",
    "        print(f'My old F1 score is: {met_total_no_entropy[2][-1]}')\n",
    "\n",
    "        # plotting ##################################################################################################\n",
    "        # general on ts\n",
    "        if use_entropy:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_entropy, dataset, filename, model_name)\n",
    "        else:\n",
    "            ts_confusion_visualization(data_test, y_pred_total_no_entropy, dataset, filename, model_name)\n",
    "\n",
    "        # model-specific rendering of internal workings\n",
    "        # use y_pred_total_e (for entropy threshold) or\n",
    "        # y_pred_total_noe for non entropy threshold\n",
    "        fig = model.plot(data_test, threshold_type=threshold_type)\n",
    "        fig.write_image(\n",
    "            f'../results/imgs/{model_name}_{dataset}_{filename}_saliency_map.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
