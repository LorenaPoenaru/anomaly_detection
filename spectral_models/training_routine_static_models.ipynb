{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433563b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "import random\n",
    "import antropy as ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9e3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the indexes of non-anomalies for interpolation \n",
    "def interpolation_indexes(mylist, mynumber):\n",
    "    \n",
    "    left_neighbour = 0\n",
    "    right_neighbour = 0\n",
    "    \n",
    "    # check left neighbour\n",
    "    if((mynumber - 1) not in mylist):\n",
    "        left_neighbour = mynumber - 1\n",
    "    else:\n",
    "        min_number = mynumber\n",
    "        while min_number in mylist:\n",
    "            min_number = min_number - 1\n",
    "        left_neighbour = min_number\n",
    "    \n",
    "    # check right neighbour\n",
    "    if((mynumber + 1) not in mylist):\n",
    "        right_neighbour = mynumber + 1\n",
    "    else:\n",
    "        max_number = mynumber\n",
    "        while max_number in mylist:\n",
    "            max_number = max_number + 1\n",
    "        right_neighbour = max_number\n",
    "    \n",
    "    return left_neighbour, right_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5361d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_removal(df_train):\n",
    "    \n",
    "    # extract indexes for anomalies\n",
    "    indexes = list(df_train[df_train.is_anomaly == 1].index)\n",
    "\n",
    "    # creating a new df that replaces the anomalous samples with interpolation value\n",
    "    df = pd.DataFrame(columns = df_train.columns)\n",
    "    print(df_train.shape)\n",
    "    for i in range(0, df_train.shape[0]):\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        # add all non-anomalies\n",
    "        if df_train.is_anomaly[i] == 0:\n",
    "            df = df.append({'timestamp' : df_train.timestamp[i], 'value' : df_train.value[i], 'is_anomaly' : df_train.is_anomaly[i]},\n",
    "            ignore_index = True)\n",
    "\n",
    "        if df_train.is_anomaly[i] == 1:\n",
    "            if (i+1) < df_train.shape[0] and df_train.is_anomaly[i+1] != 1:\n",
    "                #print(i)\n",
    "                value_interpolation = (df_train.value[interpolation_indexes(indexes, i)[0]]+df_train.value[interpolation_indexes(indexes, i)[1]])/2\n",
    "\n",
    "                df = df.append({'timestamp' : df_train.timestamp[i], 'value': value_interpolation, 'is_anomaly' : 0.0},\n",
    "            ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pci'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 65\n",
    "test_window = 65\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = None\n",
    "\n",
    "# allowed delay for anomaly shifts during evaluation\n",
    "evaluation_delay = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create FFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cbcb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.0\n",
      "['C:\\\\Users\\\\oxifl\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\numpy']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(2, '../utils/')\n",
    "\n",
    "from evaluation import label_evaluation\n",
    "from fft import detect_anomalies\n",
    "from dwt_mlead import DWT_MLEAD\n",
    "from pci import PCIAnomalyDetector\n",
    "\n",
    "# fft\n",
    "ifft_parameters: int = 5\n",
    "context_window_size: int = 21\n",
    "local_outlier_threshold: float = .6\n",
    "max_anomaly_window_size: int = 50\n",
    "max_sign_change_distance: int = 10\n",
    "random_state: int = 42\n",
    "\n",
    "# dwt mlead\n",
    "start_level: int = 3\n",
    "quantile_epsilon: float = 0.01\n",
    "random_state: int = 42\n",
    "use_column_index: int = 0\n",
    "\n",
    "# from the paper on their timeseries optimal (k,p)\n",
    "# flactuates around (5, 0.95) and (7, 0.97) so we take approx them\n",
    "window_size: int = 6\n",
    "thresholding_p: float = 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9ca88",
   "metadata": {},
   "source": [
    "# Import SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92fd4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0ba1040",
   "metadata": {},
   "source": [
    "# Entropy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4924391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrop test threshold ##########################################################\n",
    "def apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, threshold):\n",
    "    try:\n",
    "        entropy = ant.svd_entropy(window['value'].tolist(), normalize=True)\n",
    "    except:\n",
    "        entropy = (boundary_bottom + boundary_up) / 2\n",
    "\n",
    "    if entropy < boundary_bottom or entropy > boundary_up:\n",
    "        extent = stats.percentileofscore(svd_entropies, entropy) / 100.0\n",
    "        extent = 1.5 - max(extent, 1.0 - extent)\n",
    "        threshold *= extent\n",
    "    \n",
    "    predictions = [1 if a > threshold else 0 for a in anomaly_scores]\n",
    "    return predictions\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with current time series: ec2_cpu_utilization_24ae8d in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_24ae8d for 0.10024690628051758\n",
      "Smoothed F1 score is: 0.15384615384615385\n",
      "My old F1 score is: 0.15384615384615385\n",
      "Working with current time series: ec2_cpu_utilization_53ea38 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_53ea38 for 0.09156227111816406\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_5f5533 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_5f5533 for 0.09632754325866699\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_77c1ca in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_77c1ca for 0.09396719932556152\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_825cc2 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_825cc2 for 0.09206151962280273\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_ac20cd in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_ac20cd for 0.09196925163269043\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_c6585a in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_c6585a for 0.08908748626708984\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_cpu_utilization_fe7f93 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_cpu_utilization_fe7f93 for 0.09216713905334473\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_disk_write_bytes_1ef3de in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_disk_write_bytes_1ef3de for 0.10205316543579102\n",
      "Smoothed F1 score is: 0.024096385542168676\n",
      "My old F1 score is: 0.024096385542168676\n",
      "Working with current time series: ec2_disk_write_bytes_c0d644 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_disk_write_bytes_c0d644 for 0.08910202980041504\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_network_in_257a54 in dataset NAB_realAWSCloudwatch\n",
      "Trained SR on ec2_network_in_257a54 for 0.0889437198638916\n",
      "Smoothed F1 score is: 0.0\n",
      "My old F1 score is: 0.0\n",
      "Working with current time series: ec2_network_in_5abac7 in dataset NAB_realAWSCloudwatch\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18796/450008669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m                 \u001b[0mstats_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'../results/scores/{model_name}_{dataset}_{training_type}_stats.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/scores/sr_NAB_realAWSCloudwatch_sliding_window_stats.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18796/450008669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m                                         \u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrift_detector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdrift_detector\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                                         data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n\u001b[1;32m---> 86\u001b[1;33m                         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                     \u001b[0mend_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\msanomalydetector\\spectral_residual.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd_entropies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__anomaly_frame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__detect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__anomaly_frame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manomaly_window\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\msanomalydetector\\spectral_residual.py\u001b[0m in \u001b[0;36m__detect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    208\u001b[0m                 \u001b[0manomaly_frames\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mext_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manomaly_frames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__detect_core\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m--> 503\u001b[1;33m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             )\n\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_extension\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;31m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\dtypes\\concat.py\u001b[0m in \u001b[0;36mconcat_compat\u001b[1;34m(to_concat, axis)\u001b[0m\n\u001b[0;32m    178\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"object\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 'Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch', 'kpi'\n",
    "for dataset in ['Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch', 'NAB_realAWSCloudwatch_window']:\n",
    "    for training_type in ['static', 'sliding_window', 'full_history']:\n",
    "        # 'sr', 'pci', 'fft'\n",
    "        for model_name in ['sr', 'dwt_mlead', 'fft']:\n",
    "            \n",
    "            try:\n",
    "                stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_entropy.csv')\n",
    "            except:\n",
    "                # set window size equals one week\n",
    "                # yahoo is hourly, nab and kpi are 5 minute\n",
    "                if dataset == 'Yahoo_A1Benchmark':\n",
    "                    anomaly_window = 168\n",
    "                else:\n",
    "                    anomaly_window = 2016\n",
    "\n",
    "                test_window = anomaly_window\n",
    "\n",
    "                if dataset == 'kpi':\n",
    "                    train_data_path = f'../datasets/kpi/train/'\n",
    "                else:\n",
    "                    train_data_path = f'../datasets/{dataset}/'\n",
    "\n",
    "                for filename in os.listdir(train_data_path):\n",
    "                    f = os.path.join(train_data_path, filename)\n",
    "                    data = pd.read_csv(f, engine='python')\n",
    "\n",
    "                    filename = filename.replace('.csv', '')\n",
    "                    print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "                    data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "    #                 data.drop_duplicates(subset=['timestamp'], keep=False, inplace=True)\n",
    "\n",
    "                    # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "                    if dataset == 'kpi':\n",
    "                        data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename + '.csv'))\n",
    "                        data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                            lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                        data['timestamp'] = data['timestamp'].apply(\n",
    "                            lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "                        # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "                        data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "                    # 50-50 train/test split\n",
    "                    data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "                    # set (re)training (== application) windwos based on type\n",
    "                    if training_type == 'static':\n",
    "                        data_in_memory_size = anomaly_window\n",
    "                    elif training_type == 'sliding_window':\n",
    "                        data_in_memory_size = data_train.shape[0]\n",
    "                    elif training_type == 'full_history':\n",
    "                        data_in_memory_size = 0\n",
    "\n",
    "\n",
    "                    # train model #######################################################################################\n",
    "                    start = time.time()\n",
    "                    fft_threshold = 0\n",
    "\n",
    "                    if model_name == 'fft':\n",
    "                        # since no threshold is provided, we are using the max on de-anomalized training set\n",
    "                        # same tichnique is sued in dynamic models\n",
    "                        data_train = train_anomaly_removal(data_train)\n",
    "                        model = detect_anomalies\n",
    "                        random.seed(random_state)\n",
    "                        np.random.seed(random_state)\n",
    "                        anomaly_scores = model(\n",
    "                                        data_train['value'].to_numpy(),\n",
    "                                        max_region_size=max_anomaly_window_size,\n",
    "                                        local_neighbor_window=context_window_size\n",
    "                                    )\n",
    "                        fft_threshold = max(anomaly_scores)\n",
    "                    elif model_name == 'pci':\n",
    "                        model = PCIAnomalyDetector(\n",
    "                            k=window_size // 2,\n",
    "                            p=thresholding_p,\n",
    "                            calculate_labels=False\n",
    "                        )\n",
    "                    elif model_name == 'sr':\n",
    "                        model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                                        threshold=sr_model_params[0], mag_window=sr_model_params[1],\n",
    "                                        score_window=sr_model_params[2], sensitivity=sr_model_params[3],\n",
    "                                        detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                                        filename=filename, drift_detector=drift_detector,\n",
    "                                        data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "                        model.fit()\n",
    "\n",
    "                    end_time = time.time()\n",
    "\n",
    "                    diff = end_time - start\n",
    "\n",
    "                    # entropy fitting ####################################################################################\n",
    "                    svd_entropies = []\n",
    "                    entropy_factor = 1.5\n",
    "\n",
    "                    for start in range(0, data_train.shape[0], anomaly_window):\n",
    "                        try:\n",
    "                            svd_entropies.append(\n",
    "                                ant.svd_entropy(data_train[start:start + anomaly_window]['value'].tolist(),\n",
    "                                                normalize=True))\n",
    "                        except Exception as e:\n",
    "                            print(str(e))\n",
    "                    \n",
    "                    mean_entropy = np.mean([v for v in svd_entropies if pd.notna(v)])\n",
    "                    boundary_bottom = mean_entropy - \\\n",
    "                                        entropy_factor * \\\n",
    "                                        np.std([v for v in svd_entropies if pd.notna(v)])\n",
    "                    boundary_up = mean_entropy + \\\n",
    "                                    entropy_factor * \\\n",
    "                                    np.std([v for v in svd_entropies if pd.notna(v)])\n",
    "\n",
    "                    # test model #########################################################################################\n",
    "\n",
    "                    batch_metrices_f1_entropy = []\n",
    "                    batch_metrices_f1_no_entropy = []\n",
    "                    y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "                    batches_with_anomalies = []\n",
    "                    idx = 0\n",
    "\n",
    "                    pred_time = []\n",
    "\n",
    "                    if for_optimization:\n",
    "                        data_in_memory = pd.DataFrame([])\n",
    "                    else:\n",
    "                        data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "                    for start in range(0, data_test.shape[0], anomaly_window):\n",
    "                        try:\n",
    "                            # current window on which TESTING and SCORING is applied\n",
    "                            window = data_test.iloc[start:start + anomaly_window]\n",
    "                            # data hold in memory, calculations and predictions are performed across this window\n",
    "                            data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                            X, y = window['value'], window['is_anomaly']\n",
    "                            if y.tolist():\n",
    "\n",
    "                                if model_name == 'fft':\n",
    "                                    anomaly_scores = model(\n",
    "                                        data_in_memory['value'].to_numpy(),\n",
    "                                        max_region_size=max_anomaly_window_size,\n",
    "                                        local_neighbor_window=context_window_size\n",
    "                                    )\n",
    "                                    # paper does not provide any way to detect anomaly threshold.\n",
    "                                    # we use same as for lstm\n",
    "                                    y_pred_noe = [1 if x > fft_threshold else 0 for x in anomaly_scores[-window.shape[0]:]]\n",
    "                                    y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "                                elif model_name == 'sr':\n",
    "                                    model.__series__ = data_in_memory\n",
    "                                    res = model.predict(data_in_memory, window.shape[0])\n",
    "                                    y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "                                    y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                                elif model_name == 'dwt_mlead':\n",
    "                                    # FROM PAPER: \n",
    "                                    # We empirically determined the setting l = 5 for the NAB data and l = 7 for the A3 data\n",
    "                                    if 'NAB' in dataset:\n",
    "                                        start_level = 5\n",
    "                                    elif 'Yahoo' in dataset:\n",
    "                                        start_level = 7\n",
    "                                    model = DWT_MLEAD(data_in_memory['value'].to_numpy(),\n",
    "                                        start_level=start_level,\n",
    "                                        quantile_boundary_type=\"percentile\",\n",
    "                                        quantile_epsilon=quantile_epsilon,\n",
    "                                        track_coefs=True\n",
    "                                    )\n",
    "                                    anomaly_scores = model.detect()\n",
    "                                    # FROM PAPER: \n",
    "                                    # We empirically determined the setting B = 3.5 for the NAB data and B = 1 for the A3 data\n",
    "                                    threshold = 1\n",
    "                                    if 'NAB' in dataset:\n",
    "                                        threshold =  3.5\n",
    "                                    y_pred_noe = [1 if x > threshold else 0 for x in anomaly_scores[-window.shape[0]:]]\n",
    "                                    y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, threshold)\n",
    "                                elif model_name == 'pci':\n",
    "                                    anomaly_scores, anomaly_labels = model.detect(data_in_memory['value'].to_numpy())\n",
    "                                    y_pred_noe = anomaly_labels[-window.shape[0]:]\n",
    "                                    # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "\n",
    "                                idx += 1\n",
    "                                y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                                y_pred_total_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_e)][:window.shape[0]]\n",
    "                                y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "                                y_pred_e = y_pred_noe[:window.shape[0]]\n",
    "\n",
    "                        except Exception as e:\n",
    "                            raise e\n",
    "\n",
    "                    # evaluate TS ########################################################################################\n",
    "\n",
    "                    # calculate batched metrics per *test_window*\n",
    "                    # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "                    # this works because we have binary classification\n",
    "                    data_reset = data_test.reset_index()['is_anomaly']\n",
    "                    for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "                        # here, met_total will be (precision, recall, f1_score, support)\n",
    "\n",
    "                        met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                                    y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "                        batch_metrices_f1_no_entropy.append(met_total[2][-1])\n",
    "\n",
    "                    met_total_no_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                                    y_pred_total_no_entropy[:data_test.shape[0]])\n",
    "                    \n",
    "                    met_total_entropy = precision_recall_fscore_support(data_test['is_anomaly'],\n",
    "                                                                    y_pred_total_entropy[:data_test.shape[0]])\n",
    "\n",
    "                    # add entry to stats #######################################################################################\n",
    "\n",
    "                    try:\n",
    "                        stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_entropy.csv')\n",
    "                    except:\n",
    "                        stats_full = pd.DataFrame([])\n",
    "\n",
    "                    smoothed_score = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                      y_pred_total_no_entropy[:data_test.shape[0]], evaluation_delay)\n",
    "                    smoothed_score_entropy = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                      y_pred_total_entropy[:data_test.shape[0]], evaluation_delay)\n",
    "                    print(f'Smoothed F1 score is: {smoothed_score}')\n",
    "\n",
    "                    stats_full = stats_full.append({\n",
    "                        'model': model_name,\n",
    "                        'ts_name': filename,\n",
    "                        'window': anomaly_window,\n",
    "                        'delay': evaluation_delay,\n",
    "                        'f1_score': met_total_no_entropy[2][-1],\n",
    "                        'f1_score_smoothed': smoothed_score,\n",
    "                        'f1_score_entropy': met_total_entropy[2][-1],\n",
    "                        'f1_score_entropy_smoothed': smoothed_score_entropy,\n",
    "                        'labels_true': data_test['is_anomaly'].tolist(),\n",
    "                        'labels_pred': y_pred_total_no_entropy[:data_test.shape[0]]\n",
    "\n",
    "                    }, ignore_index=True)\n",
    "                    stats_full.to_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_entropy.csv', index=False)\n",
    "                    print(f'My old F1 score is: {met_total_no_entropy[2][-1]}')\n",
    "\n",
    "                    # plotting ##################################################################################################\n",
    "                    # general on ts\n",
    "                    if use_entropy:\n",
    "                        ts_confusion_visualization(data_test, y_pred_total_entropy, dataset, filename, model_name)\n",
    "                    else:\n",
    "                        ts_confusion_visualization(data_test, y_pred_total_no_entropy, dataset, filename, model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e1a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
