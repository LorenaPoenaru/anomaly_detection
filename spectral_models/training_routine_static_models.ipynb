{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "433563b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a971e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sranodec in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (1.20.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (3.4.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (1.5.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (1.1.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (3.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (2.8.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cycler>=0.10->matplotlib->sranodec) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->sranodec) (41.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install sranodec\n",
    "import sranodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "import random\n",
    "import antropy as ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d9e3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the indexes of non-anomalies for interpolation \n",
    "def interpolation_indexes(mylist, mynumber):\n",
    "    \n",
    "    left_neighbour = 0\n",
    "    right_neighbour = 0\n",
    "    \n",
    "    # check left neighbour\n",
    "    if((mynumber - 1) not in mylist):\n",
    "        left_neighbour = mynumber - 1\n",
    "    else:\n",
    "        min_number = mynumber\n",
    "        while min_number in mylist:\n",
    "            min_number = min_number - 1\n",
    "        left_neighbour = min_number\n",
    "    \n",
    "    # check right neighbour\n",
    "    if((mynumber + 1) not in mylist):\n",
    "        right_neighbour = mynumber + 1\n",
    "    else:\n",
    "        max_number = mynumber\n",
    "        while max_number in mylist:\n",
    "            max_number = max_number + 1\n",
    "        right_neighbour = max_number\n",
    "    \n",
    "    return left_neighbour, right_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5361d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_removal(df_train):\n",
    "    \n",
    "    # extract indexes for anomalies\n",
    "    indexes = list(df_train[df_train.is_anomaly == 1].index)\n",
    "\n",
    "    # creating a new df that replaces the anomalous samples with interpolation value\n",
    "    df = pd.DataFrame(columns = df_train.columns)\n",
    "    for i in range(0, df_train.shape[0]):\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        # add all non-anomalies\n",
    "        if df_train.is_anomaly[i] == 0:\n",
    "            df = df.append({'timestamp' : df_train.timestamp[i], 'value' : df_train.value[i], 'is_anomaly' : df_train.is_anomaly[i]},\n",
    "            ignore_index = True)\n",
    "\n",
    "        if df_train.is_anomaly[i] == 1:\n",
    "            if (i+1) < df_train.shape[0] and df_train.is_anomaly[i+1] != 1:\n",
    "                #print(i)\n",
    "                value_interpolation = (df_train.value[interpolation_indexes(indexes, i)[0]]+df_train.value[interpolation_indexes(indexes, i)[1]])/2\n",
    "\n",
    "                df = df.append({'timestamp' : df_train.timestamp[i], 'value': value_interpolation, 'is_anomaly' : 0.0},\n",
    "            ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pci'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 65\n",
    "test_window = 65\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = None\n",
    "\n",
    "# allowed delay for anomaly shifts during evaluation\n",
    "evaluation_delay = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create FFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98cbcb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.20.0\n",
      "['C:\\\\Users\\\\oxifl\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37\\\\lib\\\\site-packages\\\\numpy']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(2, '../utils/')\n",
    "# import importlib\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# sys.path.insert(0, os.path.abspath('../module-subdirectory'))\n",
    "\n",
    "from evaluation import label_evaluation\n",
    "from fft import detect_anomalies\n",
    "from dwt_mlead import DWT_MLEAD\n",
    "from pci import PCIAnomalyDetector\n",
    "\n",
    "# fft\n",
    "ifft_parameters: int = 5\n",
    "context_window_size: int = 21\n",
    "local_outlier_threshold: float = .6\n",
    "max_anomaly_window_size: int = 50\n",
    "max_sign_change_distance: int = 10\n",
    "random_state: int = 42\n",
    "\n",
    "# dwt mlead\n",
    "start_level: int = 3\n",
    "quantile_epsilon: float = 0.01\n",
    "random_state: int = 42\n",
    "use_column_index: int = 0\n",
    "\n",
    "# from the paper on their timeseries optimal (k,p)\n",
    "# flactuates around (5, 0.95) and (7, 0.97) so we take approx them\n",
    "window_size: int = 24\n",
    "thresholding_p: float = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9ca88",
   "metadata": {},
   "source": [
    "# Import SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92fd4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ba1040",
   "metadata": {},
   "source": [
    "# Entropy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4924391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrop test threshold ##########################################################\n",
    "def apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, threshold):\n",
    "    try:\n",
    "        entropy = ant.svd_entropy(window['value'].tolist(), normalize=True)\n",
    "    except:\n",
    "        entropy = (boundary_bottom + boundary_up) / 2\n",
    "\n",
    "    if entropy < boundary_bottom or entropy > boundary_up:\n",
    "        extent = stats.percentileofscore(svd_entropies, entropy) / 100.0\n",
    "        extent = 1.5 - max(extent, 1.0 - extent)\n",
    "        threshold *= extent\n",
    "    \n",
    "    predictions = [1 if a > threshold else 0 for a in anomaly_scores[-window.shape[0]:]]\n",
    "    return predictions\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377e3842",
   "metadata": {},
   "source": [
    "# Entropy drift detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a13d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyhht\n",
    "# from drift_detection.detect_drift import DriftDetector\n",
    "# from drift_detection.ETFE import ETFE\n",
    "from drift_detection.FEDD import FEDD\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "# import rpy2.robjects as robjects\n",
    "# from rpy2.robjects import pandas2ri\n",
    "\n",
    "\n",
    "# class DriftDetectorT(ETFE):\n",
    "\n",
    "#     def __init__(self, entropy_type='SampEn'):\n",
    "#         super().__init__(entropy_type)\n",
    "#         entropy_delta = 0.6\n",
    "        \n",
    "\n",
    "#     def process_train_data(self, data):\n",
    "#         for point in data:\n",
    "#             entropy = self.feed(point)\n",
    "\n",
    "#     def is_drifted(self, window):\n",
    "#         entropies = []\n",
    "\n",
    "#         for point in window:\n",
    "#             entropy = self.feed(point)\n",
    "#             if entropy:\n",
    "#                 entropies.append(entropy)\n",
    "\n",
    "#         print(entropies)\n",
    "#         df = pd.DataFrame([])\n",
    "#         df['entropy'] = entropies\n",
    "#         df.to_csv('entropies.csv')\n",
    "#         if entropies:\n",
    "\n",
    "#             glr_test_stat = subprocess.call(\"Rscript GLR.R\", shell=True)\n",
    "#             print(glr_test_stat)\n",
    "# #             if glr_test_stat > self.entropy_delta:\n",
    "# #                 # drift occured\n",
    "# #                 return True\n",
    "#         return False\n",
    "    \n",
    "\n",
    "# dd = DriftDetectorT()\n",
    "dd = FEDD()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fft\n",
      "static\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.007017543859649122\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.011090573012939002\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.010989010989010988\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13282732447817838\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.6403385049365302\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.09386281588447652\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0472972972972973\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.022181146025878003\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.23420074349442382\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02027027027027027\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0034542314335060447\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0070052539404553416\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006557377049180327\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02185792349726776\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0060975609756097554\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.09039548022598871\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.007017543859649122\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02380952380952381\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.00966183574879227\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.026200873362445417\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.004405286343612335\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01937984496124031\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.27538726333907054\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.00718132854578097\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13991769547325103\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01892744479495268\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.00437636761487965\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.00617283950617284\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01620745542949757\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.011214953271028037\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02593192868719611\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0034542314335060447\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.010434782608695651\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01050788091068301\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.007561436672967863\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.020583190394511147\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "0.007905138339920948\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01615508885298869\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13680781758957655\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.04983388704318936\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.06097560975609757\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.05479452054794521\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.015113350125944584\n",
      "[0.035005405617817634, 0.046969496901249075, 0.046969496901249075, 0.046969496901249075, 0.046969496901249075, 0.046969496901249075, 0.046969496901249075, 0.046969496901249075]\n",
      "[0.021495170898191033, 0.029075976051636466, 0.029075976051636466, 0.029075976051636466, 0.029075976051636466, 0.029075976051636466, 0.029075976051636466, 0.029075976051636466]\n",
      "[0.42306845662778336, 0.4676315863750733, 0.4676315863750733, 0.4676315863750733, 0.4676315863750733, 0.4676315863750733, 0.4676315863750733, 0.4676315863750733]\n",
      "======================================\n",
      "sliding_window\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006097560975609756\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006289308176100629\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01056338028169014\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.010582010582010581\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.013544018058690745\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13461538461538464\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.4648985959438377\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.11042944785276072\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01895734597156398\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.2978723404255319\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01840490797546012\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0030349013657056142\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.06312292358803986\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006791171477079796\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.008421052631578947\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02054794520547945\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.10043668122270744\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006289308176100629\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.018348623853211007\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.016574585635359115\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02127659574468085\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0037243947858473\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.03289473684210526\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.23952095808383236\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.008620689655172414\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.061818181818181814\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.003174603174603175\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.007130124777183601\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.014792899408284025\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.009966777408637873\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.029950083194675545\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0030349013657056142\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.004106776180698152\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0034965034965034965\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.008875739644970414\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02259887005649718\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "0.006993006993006993\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.014925373134328358\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.07216494845360824\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.04552352048558422\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.1183431952662722\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13333333333333333\n",
      "[0.032866980706233055, 0.03970401243779679, 0.03970401243779679, 0.042077084316895025, 0.042077084316895025, 0.046315856256172606, 0.046837169678556036, 0.046837169678556036]\n",
      "[0.02019654172820473, 0.024887691655315552, 0.024887691655315552, 0.026176701153280274, 0.026176701153280274, 0.028656868543756233, 0.028924136333811475, 0.028924136333811475]\n",
      "[0.4247257368926473, 0.4425529357404932, 0.4425529357404932, 0.4574783088748216, 0.4574783088748216, 0.47203515427743814, 0.4825707117840229, 0.4825707117840229]\n",
      "======================================\n",
      "full_history\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0061068702290076335\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006633499170812604\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.012320328542094456\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.013245033112582783\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.014018691588785047\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.13461538461538464\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.5206422018348623\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.08737864077669902\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.018808777429467086\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.018181818181818184\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0030627871362940277\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.22058823529411764\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0072072072072072065\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0163265306122449\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.020304568527918784\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.10478359908883826\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.006644518272425249\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01824817518248175\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02272727272727273\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0037174721189591076\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.03194888178913738\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.2380952380952381\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.05821917808219178\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.003125\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.007142857142857143\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.01483679525222552\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.0\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.010714285714285714\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.02922077922077922\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.003053435114503817\n",
      "0\n",
      "168\n",
      "336\n",
      "504\n",
      "672\n",
      "0.004434589800443459\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 'Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch', 'kpi'\n",
    "# 'sliding_window', 'full_history'\n",
    "for model_name in ['fft']:\n",
    "    print(model_name)\n",
    "    for dataset in ['Yahoo_A1Benchmark']:\n",
    "        for training_type in ['static', 'sliding_window', 'full_history']:\n",
    "                        # less than period\n",
    "                        amp_window_size=1300\n",
    "                        # (maybe) as same as period\n",
    "                        series_window_size=3\n",
    "                        # a number enough larger than period\n",
    "                        score_window_size=1400\n",
    "\n",
    "                        print(training_type)\n",
    "                        max_f1 = 0.0\n",
    "                        best_combo = []\n",
    "                        avg_f1 = 0.0\n",
    "                        avg_scs = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "                        avg_prec = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "                        avg_rec = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "                        act = 0\n",
    "                        minl = float('inf')\n",
    "                        maxlen = 1440\n",
    "                        # 'pci',  'dwt_mlead', 'fft', 'sr'\n",
    "                        # set window size equals one week\n",
    "                        # yahoo is hourly, nab and kpi are 5 minute\n",
    "                        if dataset == 'Yahoo_A1Benchmark':\n",
    "                            anomaly_window = 168\n",
    "                        else:\n",
    "                            anomaly_window = 1440\n",
    "\n",
    "                        try:\n",
    "                            stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv')\n",
    "                        except:\n",
    "\n",
    "                            test_window = anomaly_window\n",
    "\n",
    "                            if dataset == 'kpi':\n",
    "                                train_data_path = f'../datasets/kpi/train/'\n",
    "                            else:\n",
    "                                train_data_path = f'../datasets/{dataset}/'\n",
    "\n",
    "                            for filename in os.listdir(train_data_path):\n",
    "                                f = os.path.join(train_data_path, filename)\n",
    "                                data = pd.read_csv(f, engine='python')\n",
    "\n",
    "                                filename = filename.replace('.csv', '')\n",
    "                                # print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "                                data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "\n",
    "                                # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "                                if dataset == 'kpi':\n",
    "                                    data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename + '.csv'))\n",
    "                                    data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                                        lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                                    data['timestamp'] = data['timestamp'].apply(\n",
    "                                        lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "                                    # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "                                    data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "                                # 50-50 train/test split\n",
    "                                data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "                                minl = min(minl, data.shape[0])\n",
    "                                maxlen = max(maxlen, data.shape[0])\n",
    "                                act += data['is_anomaly'].tolist().count(1)\n",
    "\n",
    "                                if data_test['is_anomaly'].tolist().count(1) == 0:\n",
    "                                    continue\n",
    "\n",
    "                                # set (re)training (== application) windwos based on type\n",
    "                                if training_type == 'static':\n",
    "                                    data_in_memory_size = anomaly_window\n",
    "                                    pci_window = anomaly_window\n",
    "                                elif training_type == 'sliding_window':\n",
    "                                    data_in_memory_size = data_train.shape[0]\n",
    "                                    pci_window = data_train.shape[0]\n",
    "                                elif training_type == 'full_history':\n",
    "                                    data_in_memory_size = 0\n",
    "\n",
    "\n",
    "                                # train model #######################################################################################\n",
    "                                start = time.time()\n",
    "                                fft_threshold = 0\n",
    "\n",
    "                                if model_name == 'fft':\n",
    "                                    # since no threshold is provided, we are using the max on de-anomalized training set\n",
    "                                    # same tichnique is sued in dynamic models\n",
    "                                    data_train = train_anomaly_removal(data_train)\n",
    "                                    model = detect_anomalies\n",
    "                                    random.seed(random_state)\n",
    "                                    np.random.seed(random_state)\n",
    "#                                     anomaly_scores = model(\n",
    "#                                                     data_train['value'].to_numpy()\n",
    "#                                                 )\n",
    "                                    fft_threshold = 0.3\n",
    "                                elif model_name == 'pci':\n",
    "                                    model = PCIAnomalyDetector(\n",
    "                                        k=pci_window // 2,\n",
    "                                        p=thresholding_p,\n",
    "                                        calculate_labels=True\n",
    "                                    )\n",
    "                                elif model_name == 'sr':\n",
    "                                    # model = AlibiSpectralResidual(\n",
    "                                    #     threshold=THRESHOLD,\n",
    "                                    #     window_amp=SCORE_WINDOW,\n",
    "                                    #     window_local=MAG_WINDOW,\n",
    "                                    #     padding_amp_method='reflect',\n",
    "                                    #     padding_local_method='reflect',\n",
    "                                    #     padding_amp_side='bilateral',\n",
    "                                    #     n_est_points=10,\n",
    "                                    #     n_grad_points=5\n",
    "                                    # )\n",
    "                                    # model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                                    #                 threshold=THRESHOLD, mag_window=MAG_WINDOW,\n",
    "                                    #                 score_window=SCORE_WINDOW, sensitivity=SENSITIVITY,\n",
    "                                    #                 detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                                    #                 filename=filename, drift_detector=drift_detector,\n",
    "                                    #                 data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "                                    # model.fit()\n",
    "                                    pass\n",
    "\n",
    "                                end_time = time.time()\n",
    "\n",
    "                                diff = end_time - start\n",
    "\n",
    "                                # (entropy) drift detection ##########################################################################\n",
    "                                # dd.process_train_data(data_train['value'].tolist())\n",
    "\n",
    "\n",
    "                                svd_entropies = []\n",
    "                                entropy_factor = 1.5\n",
    "\n",
    "                                for start in range(0, data_train.shape[0], anomaly_window):\n",
    "                                    try:\n",
    "                                        svd_entropies.append(\n",
    "                                            ant.svd_entropy(data_train[start:start + anomaly_window]['value'].tolist(),\n",
    "                                                            normalize=True))\n",
    "                                    except Exception as e:\n",
    "                                        print(str(e))\n",
    "\n",
    "                                mean_entropy = np.mean([v for v in svd_entropies if pd.notna(v)])\n",
    "                                boundary_bottom = mean_entropy - \\\n",
    "                                                    entropy_factor * \\\n",
    "                                                    np.std([v for v in svd_entropies if pd.notna(v)])\n",
    "                                boundary_up = mean_entropy + \\\n",
    "                                                entropy_factor * \\\n",
    "                                                np.std([v for v in svd_entropies if pd.notna(v)])\n",
    "\n",
    "                                # test model #########################################################################################\n",
    "\n",
    "                                batch_metrices_f1_entropy = []\n",
    "                                batch_metrices_f1_no_entropy = []\n",
    "                                y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "                                batches_with_anomalies = []\n",
    "                                idx = 0\n",
    "\n",
    "                                pred_time = []\n",
    "\n",
    "                                if for_optimization:\n",
    "                                    data_in_memory = pd.DataFrame([])\n",
    "                                else:\n",
    "                                    data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "                                for start in range(0, data_test.shape[0], anomaly_window):\n",
    "                                    print(start)\n",
    "                                    try:\n",
    "                                        # current window on which TESTING and SCORING is applied\n",
    "                                        window = data_test.iloc[start:start + anomaly_window]\n",
    "                                        # data hold in memory, calculations and predictions are performed across this window\n",
    "                                        data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                                        X, y = window['value'], window['is_anomaly']\n",
    "\n",
    "                                        if model_name == 'fft':\n",
    "                                            anomaly_scores = model(\n",
    "                                                     data_in_memory['value'].to_numpy(),\n",
    "                                                     ifft_parameters = 5,\n",
    "                                                     local_neighbor_window = amp_window_size,\n",
    "                                                     local_outlier_threshold = .1,\n",
    "                                                     max_region_size = score_window_size,\n",
    "                                                     max_sign_change_distance = 10,\n",
    "                                                )\n",
    "                                            # paper does not provide any way to detect anomaly threshold.\n",
    "                                            # we use same as for lstm\n",
    "#                                             print(fft_threshold)\n",
    "#                                             print(anomaly_scores[-window.shape[0]:])\n",
    "#                                             print(np.percentile(anomaly_scores, 75))\n",
    "#                                             print(anomaly_scores)\n",
    "#                                             index_changes = np.where(anomaly_scores > np.percentile(anomaly_scores, ))[0]\n",
    "#                                             y_pred_noe = [1 if i in index_changes else 0 for i in range(len(anomaly_scores))][-window.shape[0]:]\n",
    "                                            y_pred_noe = [1 if x > fft_threshold else 0 for x in anomaly_scores[-window.shape[0]:]]\n",
    "                                            # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "                                        elif model_name == 'sr':\n",
    "                                            # model.__series__ = data_in_memory\n",
    "                                            # res = model.predict(data_in_memory, window.shape[0])\n",
    "                                            # y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "                                            # # y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                                            # preds = model.predict(\n",
    "                                            #                 X,\n",
    "                                            #                 t=X['timestamp'],  # array with timesteps, assumes dt=1 between observations if omitted\n",
    "                                            #                 return_instance_score=True\n",
    "                                            #             )\n",
    "                                            spec = sranodec.Silency(amp_window_size, series_window_size, score_window_size)\n",
    "                                            score = spec.generate_anomaly_score(data_in_memory['value'].to_numpy())\n",
    "                                            index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "                                            y_pred_noe = [1 if i in index_changes else 0 for i in range(len(score))][-window.shape[0]:]\n",
    "                                            # preds = preds['is_outlier']\n",
    "                                        elif model_name == 'dwt_mlead':\n",
    "                                            # FROM PAPER: \n",
    "                                            # We empirically determined the setting l = 5 for the NAB data and l = 7 for the A3 data\n",
    "                                            if 'NAB' in dataset:\n",
    "                                                start_level = 5\n",
    "                                            elif 'Yahoo' in dataset:\n",
    "                                                start_level = 7\n",
    "                                            model = DWT_MLEAD(data_in_memory['value'].to_numpy(),\n",
    "                                                start_level=start_level,\n",
    "                                                quantile_boundary_type=\"percentile\",\n",
    "                                                quantile_epsilon=quantile_epsilon,\n",
    "                                                track_coefs=True\n",
    "                                            )\n",
    "                                            anomaly_scores = model.detect()\n",
    "                                            # FROM PAPER: \n",
    "                                            # We empirically determined the setting B = 3.5 for the NAB data and B = 1 for the A3 data\n",
    "                                            threshold = 1\n",
    "                                            if 'NAB' in dataset:\n",
    "                                                threshold =  3.5\n",
    "                                            y_pred_noe = [1 if x > threshold else 0 for x in anomaly_scores[-window.shape[0]:]]\n",
    "                                            # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, threshold)\n",
    "                                        elif model_name == 'pci':\n",
    "                                            model = PCIAnomalyDetector(\n",
    "                                                k=data_in_memory.shape[0] // 2,\n",
    "                                                p=thresholding_p,\n",
    "                                                calculate_labels=True\n",
    "                                            )\n",
    "                                            anomaly_scores, anomaly_labels = model.detect(data_in_memory['value'].to_numpy())\n",
    "                                            y_pred_noe = anomaly_labels[-window.shape[0]:]\n",
    "                                            # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "\n",
    "                                        idx += 1\n",
    "                                        y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                                        # y_pred_total_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_e)][:window.shape[0]]\n",
    "                                        y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "                                        # y_pred_e = y_pred_noe[:window.shape[0]]\n",
    "\n",
    "                                    except Exception as e:\n",
    "                                        raise e\n",
    "\n",
    "                                # evaluate TS ########################################################################################\n",
    "\n",
    "                                # calculate batched metrics per *test_window*\n",
    "                                # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "                                # this works because we have binary classification\n",
    "                                data_reset = data_test.reset_index()['is_anomaly']\n",
    "                                for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "                                    # here, met_total will be (precision, recall, f1_score, support)\n",
    "\n",
    "                                    met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                                                y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "                                    batch_metrices_f1_no_entropy.append(met_total[2][-1])\n",
    "\n",
    "                                score = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                                y_pred_total_no_entropy[:data_test.shape[0]], 0)\n",
    "                                # score_entropy = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                #                                   y_pred_total_entropy[:data_test.shape[0]], 0)\n",
    "\n",
    "                                # add entry to stats #######################################################################################\n",
    "\n",
    "                                try:\n",
    "                                    stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv')\n",
    "                                except:\n",
    "                                    stats_full = pd.DataFrame([])\n",
    "\n",
    "                                res = {\n",
    "                                    'model': model_name,\n",
    "                                    'ts_name': filename,\n",
    "                                    'precision': met_total[0][-1],\n",
    "                                    'recall': met_total[1][-1],\n",
    "            #                         'window': anomaly_window,\n",
    "            #                         'delay': evaluation_delay,\n",
    "                                    # 'f1_score_entropy': score_entropy,\n",
    "                                    # 'f1_score_entropy_smoothed': smoothed_score_entropy,\n",
    "            #                         'labels_true': data_test['is_anomaly'].tolist(),\n",
    "            #                         'labels_pred': y_pred_total_no_entropy[:data_test.shape[0]]\n",
    "\n",
    "                                }\n",
    "\n",
    "                                for delay in range(8):\n",
    "                                    val, pr, r = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                                y_pred_total_no_entropy[:data_test.shape[0]], delay)\n",
    "                                    avg_scs[delay] += val\n",
    "                                    avg_prec[delay] += pr\n",
    "                                    avg_rec[delay] += r\n",
    "                                    if delay == 0:\n",
    "                                        avg_f1 += val\n",
    "                                    res[f'f1_score_{delay}'] = val\n",
    "                                    if delay == 0:\n",
    "                                        print(val)\n",
    "\n",
    "            #                     smoothed_score = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "#             #                                                       y_pred_total_no_entropy[:data_test.shape[0]], evaluation_delay)\n",
    "#                                 smoothed_score_entropy = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "#                                                                   y_pred_total_no_entropy[:data_test.shape[0]], 0)\n",
    "#                                 print(smoothed_score_entropy)\n",
    "            #                     print(f'Smoothed F1 score is: {smoothed_score}')\n",
    "\n",
    "                                stats_full = stats_full.append(res, ignore_index=True)\n",
    "                                # stats_full.to_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv', index=False)\n",
    "                                # print(f'My old F1 score is: {met_total[2][-1]}')\n",
    "\n",
    "                        avg_f1 /= len(os.listdir(train_data_path))\n",
    "                        for i in range(8):\n",
    "                            avg_scs[i] = avg_scs[i] / len(os.listdir(train_data_path))\n",
    "                            avg_prec[i] = avg_prec[i] / len(os.listdir(train_data_path))\n",
    "                            avg_rec[i] = avg_rec[i] / len(os.listdir(train_data_path))\n",
    "\n",
    "                        print(avg_scs)\n",
    "                        print(avg_prec)\n",
    "                        print(avg_rec)\n",
    "                        print('======================================')\n",
    "\n",
    "                # plotting ##################################################################################################\n",
    "                # general on ts\n",
    "                # if use_entropy:\n",
    "                #     ts_confusion_visualization(data_test, y_pred_total_entropy, dataset, filename, model_name)\n",
    "                # else:\n",
    "                #     ts_confusion_visualization(data_test, y_pred_total_no_entropy, dataset, filename, model_name)\n",
    "                # if model_name == 'sr':\n",
    "                #     fig = model.plot(data_test,  threshold_type='static')\n",
    "                #     fig.write_image(f'../results/imgs/{model_name}_{dataset}_{training_type}_{filename}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76479262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ab3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
