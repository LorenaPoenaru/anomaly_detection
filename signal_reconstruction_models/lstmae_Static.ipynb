{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2aae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the indexes of non-anomalies for interpolation \n",
    "def interpolation_indexes(mylist, mynumber):\n",
    "    \n",
    "    left_neighbour = 0\n",
    "    right_neighbour = 0\n",
    "    \n",
    "    # check left neighbour\n",
    "    if((mynumber - 1) not in mylist):\n",
    "        left_neighbour = mynumber - 1\n",
    "    else:\n",
    "        min_number = mynumber\n",
    "        while min_number in mylist:\n",
    "            min_number = min_number - 1\n",
    "        left_neighbour = min_number\n",
    "    \n",
    "    # check right neighbour\n",
    "    if((mynumber + 1) not in mylist):\n",
    "        right_neighbour = mynumber + 1\n",
    "    else:\n",
    "        max_number = mynumber\n",
    "        while max_number in mylist:\n",
    "            max_number = max_number + 1\n",
    "        right_neighbour = max_number\n",
    "    \n",
    "    return left_neighbour, right_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c8acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    L1 = LSTM(16, activation='relu', return_sequences=True, \n",
    "            kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(8, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X_train.shape[1])(L2)\n",
    "    L4 = LSTM(8, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X_train.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_removal(df_train):\n",
    "    \n",
    "    # extract indexes for anomalies\n",
    "    indexes = list(df_train[df_train.is_anomaly == 1].index)\n",
    "\n",
    "    # creating a new df that replaces the anomalous samples with interpolation value\n",
    "    df = pd.DataFrame(columns = df_train.columns)\n",
    "    for i in range(0, len(df_train)):\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        # add all non-anomalies\n",
    "        if(df_train.is_anomaly[i] == 0):\n",
    "            df = df.append({'timestamp' : df_train.timestamp[i], 'value' : df_train.value[i], 'is_anomaly' : df_train.is_anomaly[i]},\n",
    "            ignore_index = True)\n",
    "\n",
    "        if(df_train.is_anomaly[i]==1):\n",
    "            if(df_train.is_anomaly[i+1]!=1):\n",
    "                #print(i)\n",
    "                value_interpolation = (df_train.value[interpolation_indexes(indexes, i)[0]]+df_train.value[interpolation_indexes(indexes, i)[1]])/2\n",
    "\n",
    "                df = df.append({'timestamp' : df_train.timestamp[i], 'value': value_interpolation, 'is_anomaly' : 0.0},\n",
    "            ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ae05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_computing_max(X_train):\n",
    "    X_train_pred = model.predict(X_train, verbose=0)\n",
    "    train_mae_loss_avg = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
    "    max_threshold = np.max(train_mae_loss_avg)\n",
    "    return max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e4295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss_predictions(X_test):\n",
    "    X_test_pred = model.predict(X_test, verbose=0)\n",
    "    mae_loss = np.mean(np.abs(X_test_pred-X_test), axis=1)\n",
    "    return mae_loss, X_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac4b18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_labels(mae_loss, threshold):\n",
    "    predicted_test_label = []\n",
    "    for i in range(0, len(test_mae_loss)):\n",
    "        if(test_mae_loss[i][0]>(threshold)):\n",
    "            predicted_test_label.append(1)\n",
    "        else:\n",
    "            predicted_test_label.append(0)\n",
    "    return predicted_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo\n",
    "#path_files = '../../../Documents/phd_related/data_sets_concept_drift/anomaly_detection/Yahoo_A1Benchmark/'\n",
    "# NAB\n",
    "path_files = '../../../Documents/phd_related/data_sets_concept_drift/anomaly_detection/NAB/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b714d4d",
   "metadata": {},
   "source": [
    "## Extract all file names corresponding to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5156220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names = []\n",
    "for i in os.listdir(path_files):\n",
    "    ts_names.append(str(i.split('.csv')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb7b8a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_results_details = pd.DataFrame(columns = ['TS_name', 'lstmae_reconstruction_loss'])\n",
    "df_final_results = pd.DataFrame(columns = ['TS_name', 'Labels_True', 'Labels_Pred', 'Test_Size', 'Model'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "\n",
    "for ts_name in tqdm(ts_names):\n",
    "    \n",
    "    print(ts_name)\n",
    "    # path to train/test\n",
    "    filename = path_files+ts_name+\".csv\"\n",
    "    #print(filename)\n",
    "    \n",
    "    # read ts\n",
    "    df = pd.read_csv(filename)\n",
    "    #print(len(df))\n",
    "    \n",
    "    # split into train and test\n",
    "    init_train = df[0:math.floor(len(df)/2)]\n",
    "    #print(len(init_train))\n",
    "    init_test = df[math.floor(len(df)/2):]\n",
    "    #print(len(init_test))\n",
    "    \n",
    "    df_train = init_train\n",
    "    df_test = init_test\n",
    "    \n",
    "    \n",
    "    # remove anomalies from train to prepare LSTM\n",
    "    # all anomalies are replaced by the interpolation of their closest non-anomalous neighbours\n",
    "    df_train = train_anomaly_removal(df_train)\n",
    "    \n",
    "    \n",
    "    # final training dataset + labels\n",
    "    label_train = df_train.is_anomaly\n",
    "    train = df_train.value\n",
    "    \n",
    "\n",
    "    # final testing dataset + labels\n",
    "    label_test = df_test.is_anomaly\n",
    "    test = df_test.value\n",
    "    \n",
    "\n",
    "    # Data preprocessing - Scaling\n",
    "    # the scaler is fit on the training data and applied on the testing data\n",
    "    train_scale = scaler.fit_transform(np.array(train).reshape(-1, 1))\n",
    "    test_scale = scaler.transform(np.array(test).reshape(-1,1))\n",
    "    \n",
    "\n",
    "    # Shape Train Data for LSTM\n",
    "    X_train = train_scale.reshape(train_scale.shape[0], 1, 1)\n",
    "\n",
    "    # Train LSTM\n",
    "    no_epochs = 50\n",
    "    batch_size = 128\n",
    "    model = lstm_model()\n",
    "    encdec = model.fit(X_train, X_train, epochs=no_epochs, batch_size=batch_size,\n",
    "                        validation_split=0.25).history\n",
    "\n",
    "    # Threshold computing\n",
    "    threshold = threshold_computing_max(X_train)\n",
    "    \n",
    "    # Shape Test Data for LSTM\n",
    "    X_test = test_scale.reshape(test_scale.shape[0], 1, 1)\n",
    "    \n",
    "    test_mae_loss, X_test_pred = reconstruction_loss_predictions(X_test)\n",
    "    \n",
    "    # Extracting Predicted Labels\n",
    "    predicted_test_label = predicted_labels(test_mae_loss, threshold)\n",
    "    \n",
    "\n",
    "    # Save Results\n",
    "    # Save reconstruction Error for each Dataset\n",
    "    df_results_details = pd.DataFrame()\n",
    "    df_results_details['TS_name'] = [ts_name]\n",
    "    df_results_details['lstmae_reconstruction_loss'] = threshold\n",
    "    df_final_results_details = df_final_results_details.append(df_results_details)\n",
    "    \n",
    "    \n",
    "    # Save Predicted Labels\n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    df_results['TS_name'] = [ts_name]\n",
    "    df_results['Labels_True'] = [list(label_test)]\n",
    "    df_results['Labels_Pred'] = [predicted_test_label]\n",
    "    df_results['Test_Size'] = len(list(label_test))\n",
    "    df_results['Model'] = 'LSTM_AE'\n",
    "    df_final_results = df_final_results.append(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432513ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_results = df_final_results.set_index([pd.Index(np.arange(len(df_final_results))), 'TS_name'])\n",
    "df_final_results_details = df_final_results_details.set_index([pd.Index(np.arange(len(df_final_results_details))), 'TS_name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc29d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final_results.to_csv('./results/df_results_lstmae_static.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8764110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
