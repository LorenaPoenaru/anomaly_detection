{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e433007",
   "metadata": {},
   "source": [
    "# Import reproduction of SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aaec186e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sranodec in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (1.20.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (1.5.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sranodec) (3.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (1.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (3.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (0.10.0)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (2.8.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from matplotlib->sranodec) (8.4.0)\n",
      "Requirement already satisfied: six in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from cycler>=0.10->matplotlib->sranodec) (1.15.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->sranodec) (41.0.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\oxifl\\appdata\\local\\programs\\python\\python37\\lib\\site-packages)\n",
      "WARNING: You are using pip version 22.0.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%pip install sranodec\n",
    "import sranodec\n",
    "# less than period\n",
    "amp_window_size=24\n",
    "# (maybe) as same as period\n",
    "series_window_size=24\n",
    "# a number enough larger than period\n",
    "score_window_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4942eeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import warnings\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import funcy\n",
    "import copy\n",
    "import random\n",
    "import antropy as ant\n",
    "\n",
    "from alibi_detect.od import SpectralResidual as AlibiSpectralResidual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a0fcc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56005c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is helper method for TS rendering with datapoints\n",
    "# visualize FP and FN on time series\n",
    "def ts_confusion_visualization(data_test, pred_val, dataset, filename, modelname):\n",
    "    x, y, true_val = data_test['timestamp'].tolist(), data_test['value'].tolist(), data_test['is_anomaly'].tolist()\n",
    "    try:\n",
    "        x = [datetime.datetime.strptime(x, '%m/%d/%Y %H:%M') for x in x]\n",
    "    except:\n",
    "        try:\n",
    "            x = [datetime.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') for x in x]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    fp = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 0 and pred_val[i] == 1]\n",
    "    fn = [(x[i], y[i]) for i in range(len(true_val)) if true_val[i] == 1 and pred_val[i] == 0]\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, y, color='grey', lw=0.5, zorder=0)\n",
    "    ax.scatter([t[0] for t in fp], [t[1] for t in fp], color='r', s=5, zorder=5)\n",
    "    ax.scatter([t[0] for t in fn], [t[1] for t in fn], color='y', s=5, zorder=5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='k', lw=2, label='Correct'),\n",
    "                       Line2D([0], [0], marker='o', color='r', markersize=5, label='FP'),\n",
    "                       Line2D([0], [0], marker='o', color='y', markersize=5, label='FN')]\n",
    "\n",
    "    ax.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "    pyplot.savefig(f'../results/imgs/{modelname}_{dataset}_{filename}.png')\n",
    "    pyplot.clf()\n",
    "    pyplot.close('all')\n",
    "    plt.close('all')\n",
    "    del fig\n",
    "    del ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d9e3c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the indexes of non-anomalies for interpolation \n",
    "def interpolation_indexes(mylist, mynumber):\n",
    "    \n",
    "    left_neighbour = 0\n",
    "    right_neighbour = 0\n",
    "    \n",
    "    # check left neighbour\n",
    "    if((mynumber - 1) not in mylist):\n",
    "        left_neighbour = mynumber - 1\n",
    "    else:\n",
    "        min_number = mynumber\n",
    "        while min_number in mylist:\n",
    "            min_number = min_number - 1\n",
    "        left_neighbour = min_number\n",
    "    \n",
    "    # check right neighbour\n",
    "    if((mynumber + 1) not in mylist):\n",
    "        right_neighbour = mynumber + 1\n",
    "    else:\n",
    "        max_number = mynumber\n",
    "        while max_number in mylist:\n",
    "            max_number = max_number + 1\n",
    "        right_neighbour = max_number\n",
    "    \n",
    "    return left_neighbour, right_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b5361d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_removal(df_train):\n",
    "    \n",
    "    # extract indexes for anomalies\n",
    "    indexes = list(df_train[df_train.is_anomaly == 1].index)\n",
    "\n",
    "    # creating a new df that replaces the anomalous samples with interpolation value\n",
    "    df = pd.DataFrame(columns = df_train.columns)\n",
    "    for i in range(0, df_train.shape[0]):\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        # add all non-anomalies\n",
    "        if df_train.is_anomaly[i] == 0:\n",
    "            df = df.append({'timestamp' : df_train.timestamp[i], 'value' : df_train.value[i], 'is_anomaly' : df_train.is_anomaly[i]},\n",
    "            ignore_index = True)\n",
    "\n",
    "        if df_train.is_anomaly[i] == 1:\n",
    "            if (i+1) < df_train.shape[0] and df_train.is_anomaly[i+1] != 1:\n",
    "                #print(i)\n",
    "                value_interpolation = (df_train.value[interpolation_indexes(indexes, i)[0]]+df_train.value[interpolation_indexes(indexes, i)[1]])/2\n",
    "\n",
    "                df = df.append({'timestamp' : df_train.timestamp[i], 'value': value_interpolation, 'is_anomaly' : 0.0},\n",
    "            ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238edff6",
   "metadata": {},
   "source": [
    "# Set hyperparameters for train flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e11bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'pci'\n",
    "\n",
    "# decide on windwos with Lorena\n",
    "anomaly_window = 65\n",
    "test_window = 65\n",
    "for_optimization = False\n",
    "\n",
    "use_drift_adapt = False\n",
    "drift_detector = None\n",
    "use_entropy = False\n",
    "threshold_type = 'static'\n",
    "if use_entropy:\n",
    "    threshold_type = 'dynamic'\n",
    "\n",
    "# TODO discuss averaging\n",
    "# class averaging type for evaluation metrics calculations\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "avg_type = None\n",
    "\n",
    "# allowed delay for anomaly shifts during evaluation\n",
    "evaluation_delay = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eaca709",
   "metadata": {},
   "source": [
    "# Create FFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "98cbcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(2, '../utils/')\n",
    "# import importlib\n",
    "# import os\n",
    "# import sys\n",
    "\n",
    "# sys.path.insert(0, os.path.abspath('../module-subdirectory'))\n",
    "\n",
    "from evaluation import label_evaluation\n",
    "from fft import detect_anomalies\n",
    "from dwt_mlead import DWT_MLEAD\n",
    "from pci import PCIAnomalyDetector\n",
    "\n",
    "# fft\n",
    "ifft_parameters: int = 5\n",
    "context_window_size: int = 21\n",
    "local_outlier_threshold: float = .6\n",
    "max_anomaly_window_size: int = 50\n",
    "max_sign_change_distance: int = 10\n",
    "random_state: int = 42\n",
    "\n",
    "# dwt mlead\n",
    "start_level: int = 3\n",
    "quantile_epsilon: float = 0.01\n",
    "random_state: int = 42\n",
    "use_column_index: int = 0\n",
    "\n",
    "# from the paper on their timeseries optimal (k,p)\n",
    "# flactuates around (5, 0.95) and (7, 0.97) so we take approx them\n",
    "window_size: int = 6\n",
    "thresholding_p: float = 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b9ca88",
   "metadata": {},
   "source": [
    "# Import SR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25e4b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SR as a module from ../utils/sr/\n",
    "import sys\n",
    "!{sys.executable} -m pip install ../utils/sr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92fd4f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from msanomalydetector import THRESHOLD, MAG_WINDOW, SCORE_WINDOW\n",
    "from msanomalydetector import DetectMode\n",
    "from msanomalydetector.spectral_residual import SpectralResidual\n",
    "\n",
    "####################################################################################\n",
    "# this code is taken from\n",
    "# https://github.com/microsoft/anomalydetector\n",
    "# with modifications to account for sliding windows and entropy thresholding\n",
    "# the modified code is worj from:\n",
    "# https://github.com/nata1y/tiny-anom-det/tree/main/models/sr\n",
    "####################################################################################\n",
    "\n",
    "# initial parameters from the paper\n",
    "sr_model_params = (THRESHOLD, MAG_WINDOW, SCORE_WINDOW, 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f703543",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0ba1040",
   "metadata": {},
   "source": [
    "# Entropy threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4924391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entrop test threshold ##########################################################\n",
    "def apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, threshold):\n",
    "    try:\n",
    "        entropy = ant.svd_entropy(window['value'].tolist(), normalize=True)\n",
    "    except:\n",
    "        entropy = (boundary_bottom + boundary_up) / 2\n",
    "\n",
    "    if entropy < boundary_bottom or entropy > boundary_up:\n",
    "        extent = stats.percentileofscore(svd_entropies, entropy) / 100.0\n",
    "        extent = 1.5 - max(extent, 1.0 - extent)\n",
    "        threshold *= extent\n",
    "    \n",
    "    predictions = [1 if a > threshold else 0 for a in anomaly_scores[-window.shape[0]:]]\n",
    "    return predictions\n",
    "##################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0940e883",
   "metadata": {},
   "source": [
    "# Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b623d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8052/155563427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                 \u001b[0mstats_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../results/scores/sr_NAB_realAWSCloudwatch_window_static_stats_1440.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2898\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8052/155563427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 227\u001b[1;33m                             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m                     \u001b[1;31m# evaluate TS ########################################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8052/155563427.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    183\u001b[0m                                 \u001b[1;31m#             )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                                 \u001b[0mspec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msranodec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSilency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamp_window_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries_window_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore_window_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m                                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_anomaly_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_in_memory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m                                 \u001b[0mindex_changes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                                 \u001b[0my_pred_noe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindex_changes\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sranodec\\silency.py\u001b[0m in \u001b[0;36mgenerate_anomaly_score\u001b[1;34m(self, values, type)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \"\"\"\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mextended_series\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmarge_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries_window_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseries_window_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mmag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform_spectral_residual\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mextended_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sranodec\\util.py\u001b[0m in \u001b[0;36mmarge_series\u001b[1;34m(values, extend_num, forward)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmarge_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextend_num\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m     \u001b[0mnext_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextrapolate_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m     \u001b[0mextension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnext_value\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mextend_num\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sranodec\\util.py\u001b[0m in \u001b[0;36mextrapolate_next\u001b[1;34m(values)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \"\"\"\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mlast_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mslope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_value\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mslope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2904\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2905\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2906\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2908\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2898\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2900\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2902\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# 'Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch'\n",
    "# 'sliding_window', 'full_history'\n",
    "\n",
    "# loop through all models and datasets\n",
    "for model_name in ['sr', 'fft', 'pci']:\n",
    "    for dataset in ['Yahoo_A1Benchmark', 'NAB_realAWSCloudwatch']:\n",
    "        for training_type in ['static', 'sliding_window', 'full_history']:\n",
    "            print(training_type)\n",
    "            max_f1 = 0.0\n",
    "            best_combo = []\n",
    "            avg_f1 = 0.0\n",
    "            avg_scs = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "            act = 0\n",
    "            minl = float('inf')\n",
    "            maxlen = 1440\n",
    "            # 'pci',  'dwt_mlead', 'fft', 'sr'\n",
    "            # set window size equals one week\n",
    "            # yahoo is hourly, nab and kpi are 5 minute\n",
    "            if dataset == 'Yahoo_A1Benchmark':\n",
    "                anomaly_window = 168\n",
    "            else:\n",
    "                anomaly_window = 1440\n",
    "\n",
    "            try:\n",
    "                stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv')\n",
    "            except:\n",
    "\n",
    "                test_window = anomaly_window\n",
    "\n",
    "                if dataset == 'kpi':\n",
    "                    train_data_path = f'../datasets/kpi/train/'\n",
    "                else:\n",
    "                    train_data_path = f'../datasets/{dataset}/'\n",
    "\n",
    "                for filename in os.listdir(train_data_path):\n",
    "                    f = os.path.join(train_data_path, filename)\n",
    "                    data = pd.read_csv(f, engine='python')\n",
    "\n",
    "                    filename = filename.replace('.csv', '')\n",
    "                    # print(f'Working with current time series: {filename} in dataset {dataset}')\n",
    "\n",
    "                    data.rename(columns={'timestamps': 'timestamp', 'anomaly': 'is_anomaly'}, inplace=True)\n",
    "\n",
    "                    # timestamp preprocessing for kpi -- their are unix timestamps\n",
    "                    if dataset == 'kpi':\n",
    "                        data_test = pd.read_csv(os.path.join(f'../datasets/{dataset}/test/', filename + '.csv'))\n",
    "                        data_test['timestamp'] = data_test['timestamp'].apply(\n",
    "                            lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "                        data['timestamp'] = data['timestamp'].apply(\n",
    "                            lambda x: datetime.datetime.utcfromtimestamp(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "                        # kpi stores train and test in different ts -- merge them into one to follow structure\n",
    "                        data = pd.concat([data, data_test], ignore_index=True)\n",
    "\n",
    "                    # 50-50 train/test split\n",
    "                    data_train, data_test = np.array_split(data, 2)\n",
    "\n",
    "                    minl = min(minl, data.shape[0])\n",
    "                    maxlen = max(maxlen, data.shape[0])\n",
    "                    act += data['is_anomaly'].tolist().count(1)\n",
    "                    \n",
    "                    if data_test['is_anomaly'].tolist().count(1) == 0:\n",
    "                        continue\n",
    "\n",
    "                    # set (re)training (== application) windwos based on type\n",
    "                    if training_type == 'static':\n",
    "                        data_in_memory_size = anomaly_window\n",
    "                        pci_window = anomaly_window\n",
    "                    elif training_type == 'sliding_window':\n",
    "                        data_in_memory_size = data_train.shape[0]\n",
    "                        pci_window = data_train.shape[0]\n",
    "                    elif training_type == 'full_history':\n",
    "                        data_in_memory_size = 0\n",
    "\n",
    "\n",
    "                    # train model #######################################################################################\n",
    "                    start = time.time()\n",
    "                    fft_threshold = 0\n",
    "\n",
    "                    if model_name == 'fft':\n",
    "                        # since no threshold is provided, we are using the max on de-anomalized training set\n",
    "                        # same tichnique is sued in dynamic models\n",
    "                        data_train = train_anomaly_removal(data_train)\n",
    "                        model = detect_anomalies\n",
    "                        random.seed(random_state)\n",
    "                        np.random.seed(random_state)\n",
    "                        anomaly_scores = model(\n",
    "                                        data_train['value'].to_numpy()\n",
    "                                    )\n",
    "                        fft_threshold = max(anomaly_scores)\n",
    "                    elif model_name == 'pci':\n",
    "                        model = PCIAnomalyDetector(\n",
    "                            k=pci_window // 2,\n",
    "                            p=thresholding_p,\n",
    "                            calculate_labels=True\n",
    "                        )\n",
    "                    elif model_name == 'sr':\n",
    "                        # THIS IS microsoft model\n",
    "                        # model = SpectralResidual(series=data_train[['value', 'timestamp']], use_drift=use_drift_adapt,\n",
    "                        #                 threshold=THRESHOLD, mag_window=MAG_WINDOW,\n",
    "                        #                 score_window=SCORE_WINDOW, sensitivity=SENSITIVITY,\n",
    "                        #                 detect_mode=DetectMode.anomaly_only, dataset=dataset,\n",
    "                        #                 filename=filename, drift_detector=drift_detector,\n",
    "                        #                 data_in_memory_sz=data_in_memory_size, anomaly_window=anomaly_window)\n",
    "                        # model.fit()\n",
    "                        pass\n",
    "\n",
    "                    end_time = time.time()\n",
    "\n",
    "                    diff = end_time - start\n",
    "\n",
    "                    # test model #########################################################################################\n",
    "\n",
    "                    batch_metrices_f1_entropy = []\n",
    "                    batch_metrices_f1_no_entropy = []\n",
    "                    y_pred_total_no_entropy, y_pred_total_entropy = [], []\n",
    "                    batches_with_anomalies = []\n",
    "                    idx = 0\n",
    "\n",
    "                    pred_time = []\n",
    "\n",
    "                    if for_optimization:\n",
    "                        data_in_memory = pd.DataFrame([])\n",
    "                    else:\n",
    "                        data_in_memory = copy.deepcopy(data_train)\n",
    "\n",
    "                    for start in range(0, data_test.shape[0], anomaly_window):\n",
    "                        try:\n",
    "                            # current window on which TESTING and SCORING is applied\n",
    "                            window = data_test.iloc[start:start + anomaly_window]\n",
    "                            # data hold in memory, calculations and predictions are performed across this window\n",
    "                            data_in_memory = pd.concat([data_in_memory, window])[-data_in_memory_size:]\n",
    "\n",
    "                            X, y = window['value'], window['is_anomaly']\n",
    "\n",
    "                            if model_name == 'fft':\n",
    "                                anomaly_scores = model(\n",
    "                                        data_in_memory['value'].to_numpy()\n",
    "                                    )\n",
    "                                # paper does not provide any way to detect anomaly threshold.\n",
    "                                # we use same as for lstm\n",
    "                                y_pred_noe = [1 if x > fft_threshold else 0 for x in anomaly_scores[-window.shape[0]:]]\n",
    "                                # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "                            elif model_name == 'sr':\n",
    "                                # THIS IS MICROSOFT MODEL\n",
    "                                # model.__series__ = data_in_memory\n",
    "                                # res = model.predict(data_in_memory, window.shape[0])\n",
    "                                # y_pred_noe = [1 if x else 0 for x in res['isAnomaly'].tolist()]\n",
    "                                # # y_pred_e = [1 if x else 0 for x in res['isAnomaly_e'].tolist()]\n",
    "                                # preds = model.predict(\n",
    "                                #                 X,\n",
    "                                #                 t=X['timestamp'],  # array with timesteps, assumes dt=1 between observations if omitted\n",
    "                                #                 return_instance_score=True\n",
    "                                #             )\n",
    "\n",
    "                                # this is reproduction of SR\n",
    "                                spec = sranodec.Silency(amp_window_size, series_window_size, score_window_size)\n",
    "                                score = spec.generate_anomaly_score(data_in_memory)\n",
    "                                index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "                                y_pred_noe = [1 if i in index_changes else 0 for i in range(len(score))][-window.shape[0]:]\n",
    "                                # preds = preds['is_outlier']\n",
    "                            elif model_name == 'pci':\n",
    "                                model = PCIAnomalyDetector(\n",
    "                                    k=data_in_memory.shape[0] // 2,\n",
    "                                    p=thresholding_p,\n",
    "                                    calculate_labels=True\n",
    "                                )\n",
    "                                anomaly_scores, anomaly_labels = model.detect(data_in_memory['value'].to_numpy())\n",
    "                                y_pred_noe = anomaly_labels[-window.shape[0]:]\n",
    "                                # y_pred_e = apply_entropy_threshold(svd_entropies, window, boundary_bottom, boundary_up, anomaly_scores, fft_threshold)\n",
    "\n",
    "                            idx += 1\n",
    "                            y_pred_total_no_entropy += [0 if val != 1 else 1 for val in funcy.lflatten(y_pred_noe)][:window.shape[0]]\n",
    "                            y_pred_noe = y_pred_noe[:window.shape[0]]\n",
    "\n",
    "                        except Exception as e:\n",
    "                            raise e\n",
    "\n",
    "                    # evaluate TS ########################################################################################\n",
    "\n",
    "                    # calculate batched metrics per *test_window*\n",
    "                    # for test stats we calculate F1 score for eacj class but use score for anomaly label \n",
    "                    # this works because we have binary classification\n",
    "                    data_reset = data_test.reset_index()['is_anomaly']\n",
    "                    for i in range(0, len(data_test['is_anomaly']), test_window):\n",
    "                        # here, met_total will be (precision, recall, f1_score, support)\n",
    "\n",
    "                        met_total = precision_recall_fscore_support(data_reset[i:i+test_window],\n",
    "                                                                    y_pred_total_no_entropy[:data_test.shape[0]][i:i+test_window])\n",
    "                        batch_metrices_f1_no_entropy.append(met_total[2][-1])\n",
    "\n",
    "                    score = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                    y_pred_total_no_entropy[:data_test.shape[0]], 0)\n",
    "                    # add entry to stats #######################################################################################\n",
    "\n",
    "                    try:\n",
    "                        stats_full = pd.read_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv')\n",
    "                    except:\n",
    "                        stats_full = pd.DataFrame([])\n",
    "                        \n",
    "                    res = {\n",
    "                        'model': model_name,\n",
    "                        'ts_name': filename,\n",
    "                        'precision': met_total[0][-1],\n",
    "                        'recall': met_total[1][-1],\n",
    "#                         'window': anomaly_window,\n",
    "#                         'delay': evaluation_delay,\n",
    "                        # 'f1_score_entropy': score_entropy,\n",
    "                        # 'f1_score_entropy_smoothed': smoothed_score_entropy,\n",
    "#                         'labels_true': data_test['is_anomaly'].tolist(),\n",
    "#                         'labels_pred': y_pred_total_no_entropy[:data_test.shape[0]]\n",
    "\n",
    "                    }\n",
    "                    \n",
    "                    for delay in range(8):\n",
    "                        val = label_evaluation(data_test['is_anomaly'].tolist(), \n",
    "                                                    y_pred_total_no_entropy[:data_test.shape[0]], delay)\n",
    "                        avg_scs[delay] += val\n",
    "                        if delay == 0:\n",
    "                            avg_f1 += val\n",
    "                        res[f'f1_score_{delay}'] = val\n",
    "\n",
    "\n",
    "                    stats_full = stats_full.append(res, ignore_index=True)\n",
    "                    stats_full.to_csv(f'../results/scores/{model_name}_{dataset}_{training_type}_stats_{anomaly_window}.csv', index=False)\n",
    "\n",
    "                avg_f1 /= len(os.listdir(train_data_path))\n",
    "                for i in range(8):\n",
    "                    avg_scs[i] = avg_scs[i] / len(os.listdir(train_data_path))\n",
    "                \n",
    "                # printing average scores\n",
    "                print(avg_scs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76479262",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ab3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "df10a3505701b17119fcb7e3d7f2d07794f5df50b37fdb5d4118a55945757b43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
