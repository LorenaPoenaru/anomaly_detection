{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2aae0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.layers import Input, Dense, LSTM, TimeDistributed, RepeatVector\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that finds the indexes of non-anomalies for interpolation \n",
    "def interpolation_indexes(mylist, mynumber):\n",
    "    \n",
    "    left_neighbour = 0\n",
    "    right_neighbour = 0\n",
    "    \n",
    "    # check left neighbour\n",
    "    if((mynumber - 1) not in mylist):\n",
    "        left_neighbour = mynumber - 1\n",
    "    else:\n",
    "        min_number = mynumber\n",
    "        while min_number in mylist:\n",
    "            min_number = min_number - 1\n",
    "        left_neighbour = min_number\n",
    "    \n",
    "    # check right neighbour\n",
    "    if((mynumber + 1) not in mylist):\n",
    "        right_neighbour = mynumber + 1\n",
    "    else:\n",
    "        max_number = mynumber\n",
    "        while max_number in mylist:\n",
    "            max_number = max_number + 1\n",
    "        right_neighbour = max_number\n",
    "    \n",
    "    return left_neighbour, right_neighbour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27daa684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model():\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    L1 = LSTM(16, activation='relu', return_sequences=True, \n",
    "            kernel_regularizer=regularizers.l2(0.00))(inputs)\n",
    "    L2 = LSTM(8, activation='relu', return_sequences=False)(L1)\n",
    "    L3 = RepeatVector(X_train.shape[1])(L2)\n",
    "    L4 = LSTM(8, activation='relu', return_sequences=True)(L3)\n",
    "    L5 = LSTM(16, activation='relu', return_sequences=True)(L4)\n",
    "    output = TimeDistributed(Dense(X_train.shape[2]))(L5)    \n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f0282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_anomaly_removal(df_train):\n",
    "    \n",
    "    # extract indexes for anomalies\n",
    "    indexes = list(df_train[df_train.is_anomaly == 1].index)\n",
    "\n",
    "    # creating a new df that replaces the anomalous samples with interpolation value\n",
    "    df = pd.DataFrame(columns = df_train.columns)\n",
    "    for i in range(0, len(df_train)):\n",
    "\n",
    "        #print(i)\n",
    "\n",
    "        # add all non-anomalies\n",
    "        if(df_train.is_anomaly[i] == 0):\n",
    "            df = df.append({'timestamp' : df_train.timestamp[i], 'value' : df_train.value[i], 'is_anomaly' : df_train.is_anomaly[i]},\n",
    "            ignore_index = True)\n",
    "\n",
    "        if((df_train.is_anomaly[i]==1) & (i != (len(df_train)-1)) & (i != 0)):\n",
    "            if(df_train.is_anomaly[i+1]!=1):\n",
    "                \n",
    "                if((interpolation_indexes(indexes, i)[0] != -1) & (interpolation_indexes(indexes, i)[1] != -1)):\n",
    "                    value_interpolation = (df_train.value[interpolation_indexes(indexes, i)[0]]\n",
    "                                           +df_train.value[interpolation_indexes(indexes, i)[1]])/2\n",
    "\n",
    "                    df = df.append({'timestamp' : df_train.timestamp[i], 'value': value_interpolation, 'is_anomaly' : 0.0}, ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf85af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_computing_max(X_train):\n",
    "    X_train_pred = model.predict(X_train, verbose=0)\n",
    "    train_mae_loss_avg = np.mean(np.abs(X_train_pred - X_train), axis=1)\n",
    "    max_threshold = np.max(train_mae_loss_avg)\n",
    "    return max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3433298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss_predictions(X_test):\n",
    "    X_test_pred = model.predict(X_test, verbose=0)\n",
    "    mae_loss = np.mean(np.abs(X_test_pred-X_test), axis=1)\n",
    "    return mae_loss, X_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf729a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicted_labels(mae_loss, threshold):\n",
    "    predicted_test_label = []\n",
    "    for i in range(0, len(test_mae_loss)):\n",
    "        if(test_mae_loss[i][0]>(threshold)):\n",
    "            predicted_test_label.append(1)\n",
    "        else:\n",
    "            predicted_test_label.append(0)\n",
    "    return predicted_test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eebf854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo\n",
    "#path_files = '../datasets/Yahoo_A1Benchmark/'\n",
    "# NAB\n",
    "path_files = '../datasets/NAB_realAWSCloudwatch/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b714d4d",
   "metadata": {},
   "source": [
    "## Extract all file names corresponding to time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5156220",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names = []\n",
    "for i in os.listdir(path_files):\n",
    "    ts_names.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ed38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yahoo\n",
    "#retraining_batches = pd.read_csv('./yahoo_retraining_batches.csv')\n",
    "# NAB\n",
    "retraining_batches = pd.read_csv('../results/fedd_results/nab_retraining_batches.csv')\n",
    "\n",
    "retraining_batches = retraining_batches.loc[:, ~retraining_batches.columns.str.contains('^Unnamed')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5782b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_final_results_details = pd.DataFrame(columns = ['TS_name', 'lstmae_reconstruction_loss'])\n",
    "df_final_results = pd.DataFrame(columns = ['TS_name', 'Labels_True', 'Labels_Pred', 'Test_Size', 'Model'])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# for daily retraining NAB\n",
    "#window_retraining = 288\n",
    "\n",
    "# for weekly retraining NAB\n",
    "window_retraining = 2016\n",
    "\n",
    "# for weekly retraining Yahoo\n",
    "#window_retraining = 168\n",
    "\n",
    "\n",
    "for ts_name in tqdm(ts_names):\n",
    "    \n",
    "    \n",
    "    # retraining_batches extraction\n",
    "    \n",
    "    str_retraining_batches = list(retraining_batches[retraining_batches.TS_name == ts_name].Retraining_Batches)[0]\n",
    "    list_str_retraining_batches = str_retraining_batches.split(',')\n",
    "    list_retraining_batches = []\n",
    "    for i in range(0, len(list_str_retraining_batches)):\n",
    "        list_str_retraining_batches[i] = list_str_retraining_batches[i].replace('[', '')\n",
    "        list_str_retraining_batches[i] = list_str_retraining_batches[i].replace(']', '')\n",
    "        \n",
    "        if(len(list_str_retraining_batches[0])):\n",
    "            #print(list_str_retraining_batches[i])\n",
    "            list_retraining_batches.append(int(list_str_retraining_batches[i]))\n",
    "        else:\n",
    "            list_retraining_batches.append(0)\n",
    "    print(list_retraining_batches)\n",
    "    \n",
    "    \n",
    "    for n in range(0, 5):\n",
    "        \n",
    "        label_pred_complete = []\n",
    "        losses_complete = []\n",
    "\n",
    "        #print(ts_name)\n",
    "        # path to train/test\n",
    "        filename_nab = path_files+ts_name\n",
    "\n",
    "        # read ts\n",
    "        df_nab = pd.read_csv(filename_nab)\n",
    "        \n",
    "        # split into train and test\n",
    "        init_train = df_nab[0:math.floor(len(df_nab)/2)]\n",
    "        # print(len(init_train))\n",
    "        init_test = df_nab[math.floor(len(df_nab)/2):]\n",
    "        # print(len(init_test))\n",
    "        \n",
    "        for i in tqdm(range(0, (math.floor(len(init_test)/window_retraining)+1))):\n",
    "            #print('i = ', i)\n",
    "            \n",
    "            if(i==0):\n",
    "                df_train = init_train\n",
    "                df_test = init_test[(i*window_retraining):((i+1)*window_retraining)]\n",
    "            \n",
    "            \n",
    "            # remove anomalies from train to prepare LSTM\n",
    "            # all anomalies are replaced by the interpolation of their closest non-anomalous neighbours\n",
    "            df_train_preprocess = train_anomaly_removal(df_train)\n",
    "\n",
    "\n",
    "            # final training dataset + labels\n",
    "            label_train = df_train_preprocess.is_anomaly\n",
    "            train = df_train_preprocess.value\n",
    "\n",
    "\n",
    "            # final testing dataset + labels\n",
    "            label_test = df_test.is_anomaly\n",
    "            test = df_test.value\n",
    "\n",
    "            # in case the training contains all data and there is no more data left for testing\n",
    "            if(df_test.empty):\n",
    "                break\n",
    "\n",
    "\n",
    "            # Data preprocessing - Scaling\n",
    "            # the scaler is fit on the training data and applied on the testing data\n",
    "            train_scale = scaler.fit_transform(np.array(train).reshape(-1, 1))\n",
    "            test_nab_scale = scaler.transform(np.array(test).reshape(-1,1))\n",
    "\n",
    "            # Shape Train Data for LSTM\n",
    "            X_train = train_scale.reshape(train_scale.shape[0], 1, 1)\n",
    "\n",
    "            # Train LSTM\n",
    "            no_epochs = 50\n",
    "            batch_size = 128\n",
    "            model = lstm_model()\n",
    "            encdec = model.fit(X_train, X_train, epochs=no_epochs, batch_size=batch_size,\n",
    "                                validation_split=0.25).history\n",
    "\n",
    "            # Threshold computing\n",
    "            threshold = threshold_computing_max(X_train)\n",
    "\n",
    "            # Shape Test Data for LSTM\n",
    "            X_test = test_nab_scale.reshape(test_nab_scale.shape[0], 1, 1)\n",
    "\n",
    "            test_mae_loss, X_test_pred = reconstruction_loss_predictions(X_test)\n",
    "\n",
    "            # Extracting Predicted Labels\n",
    "            y_label_pred = predicted_labels(test_mae_loss, threshold)\n",
    "\n",
    "            label_pred_complete.append(y_label_pred)\n",
    "            losses_complete.append(threshold)\n",
    "            \n",
    "            \n",
    "            if((i in list_retraining_batches) and (i<(int(round(len(init_test)/window_retraining)-1)))):\n",
    "                print('batch has drift')\n",
    "                #print(i<int(round(len(init_test)/window_retraining)))\n",
    "                df_train = pd.concat([init_train, init_test[0:(i+1)*window_retraining]], ignore_index=True)\n",
    "            else:\n",
    "                print('batch does not have drift')\n",
    "                \n",
    "            df_test = init_test[((i+1)*window_retraining):((i+2)*window_retraining)]\n",
    "                \n",
    "\n",
    "        all_predicted_labels = []\n",
    "        for i in range(0, len(label_pred_complete)):\n",
    "            for j in range(0, len(label_pred_complete[i])):\n",
    "                all_predicted_labels.append(label_pred_complete[i][j])\n",
    "\n",
    "        # Save Results\n",
    "        # Save reconstruction Error for each Dataset\n",
    "        df_results_details = pd.DataFrame()\n",
    "        df_results_details['TS_name'] = [ts_name]\n",
    "        df_results_details['lstmae_reconstruction_loss'] = [losses_complete]\n",
    "        df_results_details['retraining_technique'] = 'full_history'\n",
    "        df_results_details['retraining_window'] = window_retraining\n",
    "        df_final_results_details = df_final_results_details.append(df_results_details)\n",
    "\n",
    "\n",
    "        # Save Predicted Labels\n",
    "        df_results = pd.DataFrame()\n",
    "\n",
    "\n",
    "        df_results['TS_name'] = [ts_name]\n",
    "        df_results['retraining_technique'] = 'full_history'\n",
    "        df_results['retraining_window'] = window_retraining\n",
    "        df_results['Labels_True'] = [list(init_test.is_anomaly)]\n",
    "        df_results['Labels_Pred'] = [all_predicted_labels]\n",
    "        df_results['Test_Size'] = len(list(init_test.is_anomaly))\n",
    "        df_results['Model'] = 'LSTM_AE'\n",
    "        df_final_results = df_final_results.append(df_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c2ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_results = df_final_results.reset_index(drop=True)\n",
    "df_final_results_details = df_final_results_details.reset_index(drop=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
