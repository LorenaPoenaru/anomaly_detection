{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e9b165",
   "metadata": {},
   "source": [
    "Copyright (C) Microsoft Corporation. All rights reserved.​\n",
    "\n",
    "Microsoft Corporation (\"Microsoft\") grants you a nonexclusive, perpetual, royalty-free right to use, copy, and modify the software code provided by us (\"Software Code\"). You may not sublicense the Software Code or any use of it (except to your affiliates and to vendors to perform work on your behalf) through distribution, network access, service agreement, lease, rental, or otherwise. This license does not purport to express any claim of ownership over data you may have shared with Microsoft in the creation of the Software Code. Unless applicable law gives you more rights, Microsoft reserves all other rights not expressly granted herein, whether by implication, estoppel or otherwise.\n",
    "\n",
    "THE SOFTWARE CODE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL MICROSOFT OR ITS LICENSORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THE SOFTWARE CODE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edcb55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(2, '../utils/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from net import *\n",
    "from utils import *\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from msanomalydetector.util import average_filter\n",
    "from competition_metric import evaluate_for_all_series\n",
    "import torch\n",
    "import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37180fc4",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "df7858d9",
   "metadata": {},
   "source": [
    "## Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7d5d2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gen():\n",
    "    def __init__(self, win_siz, step, nums):\n",
    "        self.control = 0\n",
    "        self.win_siz = win_siz\n",
    "        self.step = step\n",
    "        self.number = nums\n",
    "\n",
    "    def generate_train_data(self, value, back_k=5):\n",
    "        def normalize(a):\n",
    "            amin = np.min(a)\n",
    "            amax = np.max(a)\n",
    "            a = (a - amin) / (amax - amin + 1e-5)\n",
    "            return 3 * a\n",
    "\n",
    "        if back_k <= 5:\n",
    "            back = back_k\n",
    "        else:\n",
    "            back = 5\n",
    "        length = len(value)\n",
    "        tmp = []\n",
    "        for pt in range(self.win_siz, length - back, self.step):\n",
    "            head = max(0, pt - self.win_siz)\n",
    "            tail = min(length - back, pt)\n",
    "            data = np.array(value[head:tail])\n",
    "            data = data.astype(np.float64)\n",
    "\n",
    "            # print('Initial', data)\n",
    "            data = normalize(data)\n",
    "            # print('Normalized', data)\n",
    "            num = np.random.randint(1, self.number)\n",
    "            ids = np.random.choice(self.win_siz, num, replace=False)\n",
    "            lbs = np.zeros(self.win_siz, dtype=np.int64)\n",
    "            if (self.win_siz - 6) not in ids:\n",
    "                self.control += np.random.random()\n",
    "            else:\n",
    "                self.control = 0\n",
    "            if self.control > 100:\n",
    "                ids[0] = self.win_siz - 6\n",
    "                self.control = 0\n",
    "            mean = np.mean(data)\n",
    "            dataavg = average_filter(data)\n",
    "            var = np.var(data)\n",
    "            for id in ids:\n",
    "                data[id] += (dataavg[id] + mean) * np.random.randn() * min((1 + var), 10)\n",
    "                lbs[id] = 1\n",
    "            tmp.append([data.tolist(), lbs.tolist()])\n",
    "        return tmp\n",
    "\n",
    "\n",
    "def auto(dic):\n",
    "    path_auto = os.getcwd() + '/auto.json'\n",
    "    auto = {}\n",
    "    for item, value in dic:\n",
    "        if value != None:\n",
    "            auto[item] = value\n",
    "    with open(path_auto, 'w+') as f:\n",
    "        json.dump(auto, f)\n",
    "\n",
    "\n",
    "def get_path(dataset_path, t):\n",
    "    files_path = []\n",
    "    if t == 'data_train' or t == 'data_test':\n",
    "        dir_data = os.getcwd() + '/' + dataset_path + '/' + t\n",
    "        files = os.listdir(dir_data)\n",
    "        files_path += [dir_data + '/' + f for f in files if not str(f).endswith('.gitkeep')]\n",
    "    else:\n",
    "        print('Invalid option')\n",
    "    return files_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c766c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd35aad",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cefb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto(epoch):\n",
    "    path_auto = os.getcwd() + '/auto.json'\n",
    "    with open(path_auto, 'r+') as f:\n",
    "        store = json.load(f)\n",
    "    data = store['data']\n",
    "    window = store['window']\n",
    "    store['epoch'] = epoch\n",
    "    with open(path_auto, 'w+') as f:\n",
    "        json.dump(store, f)\n",
    "    return data, window"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1454bc5c",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3a783e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto():\n",
    "    path_auto = os.getcwd() + '/auto.json'\n",
    "    with open(path_auto, 'r+') as f:\n",
    "        store = json.load(f)\n",
    "    window = store['window']\n",
    "    epoch = store['epoch']\n",
    "    return window, epoch\n",
    "\n",
    "\n",
    "def getfid(path):\n",
    "    return path.split('/')[-1]\n",
    "\n",
    "\n",
    "#def get_score(files, thres, option):\n",
    "def get_score(df_test, ts, thres, option):\n",
    "\n",
    "\n",
    "    total_time = 0\n",
    "    results = []\n",
    "    savedscore = []\n",
    "\n",
    "\n",
    "    in_timestamp = df_test['timestamp']\n",
    "    in_value = df_test['value']\n",
    "    in_label = df_test['is_anomaly']\n",
    "\n",
    "\n",
    "    length = len(in_timestamp)\n",
    "\n",
    "\n",
    "    if model == 'sr_cnn' and len(in_value) < window:\n",
    "        print(\"length is shorter than win_size\", len(in_value), window)\n",
    "    time_start = time.time()\n",
    "    timestamp, label, pre, scores = models[model](np.array(in_timestamp), np.array(in_value), np.array(in_label),\n",
    "                                                  window, net, option, thres)\n",
    "    \n",
    "\n",
    "    \n",
    "    time_end = time.time()\n",
    "    total_time += time_end - time_start\n",
    "    results.append([timestamp, label, pre, ts])\n",
    "    savedscore.append([label, scores, ts, timestamp])\n",
    "    return total_time, results, savedscore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55ed4330",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd76a626",
   "metadata": {},
   "source": [
    "#### Parser Values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "775e3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAB\n",
    "data_train = '../datasets/data_nab/'\n",
    "window = 64 # window_nab = 64, window_yahoo = 64\n",
    "step = 8 # step_nab = 8, step_yahoo = 8\n",
    "seed = 54321\n",
    "num = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63b6d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-5 # lr_nab = 1e-5, lr_yahoo = 1e-6\n",
    "load = False\n",
    "save = 'snapshot'\n",
    "epoch = 10\n",
    "batch_size = 256\n",
    "num_workers = 8\n",
    "model = 'sr_cnn'\n",
    "auto = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb551c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAB\n",
    "data_test = '../datasets/data_nab'\n",
    "\n",
    "epoch = 10\n",
    "model_path_snapshot = 'snapshot'\n",
    "delay = 0\n",
    "thres = 0.95\n",
    "auto = False\n",
    "missing_option = 'anomaly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23d0ad3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names_train = get_path(data_train, 'data_train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e326aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names_test = get_path(data_test, 'data_test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6ba1f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names = []\n",
    "for i in os.listdir(data_test+'/data_train/'):\n",
    "    ts_names.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76feda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "38dd1b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [00:01<00:00, 59.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# for daily retraining NAB\n",
    "#window_retraining = 288\n",
    "\n",
    "# for weekly retraining NAB\n",
    "window_retraining = 2016\n",
    "\n",
    "# for weekly retraining Yahoo\n",
    "#window_retraining = 168\n",
    "\n",
    "\n",
    "\n",
    "dataset_split_dict = {}\n",
    "\n",
    "for ts in tqdm.tqdm(range(0, len(ts_names_train))):\n",
    "\n",
    "    begin_train_timestamp = []\n",
    "    end_train_timestamp = []\n",
    "\n",
    "    begin_test_timestamp = []\n",
    "    end_test_timestamp = []\n",
    "\n",
    "    init_train = pd.read_csv(ts_names_train[ts])\n",
    "    init_test = pd.read_csv(ts_names_test[ts])\n",
    "    \n",
    "\n",
    "\n",
    "    for i in range(0, (math.floor(len(init_test)/window_retraining)+1)):\n",
    "        \n",
    "\n",
    "        if(i == 0):\n",
    "            df_train = init_train\n",
    "        else:\n",
    "            df_train = pd.concat([init_train, init_test[0:window_retraining*i]], ignore_index=True)\n",
    "\n",
    "        if(i == (round(len(init_test)/window_retraining))):\n",
    "            df_test = init_test[(i)*window_retraining:]\n",
    "        else:\n",
    "            df_test = init_test[(i*window_retraining):((i+1)*window_retraining)]\n",
    "            \n",
    "        \n",
    "       \n",
    "        \n",
    "        if(len(df_test)):\n",
    "            begin_train_timestamp.append(df_train.iloc[0].timestamp)\n",
    "            end_train_timestamp.append(df_train.iloc[len(df_train)-1].timestamp)\n",
    "            begin_test_timestamp.append(df_test.iloc[0].timestamp)\n",
    "            end_test_timestamp.append(df_test.iloc[len(df_test)-1].timestamp)\n",
    "    \n",
    "    \n",
    "\n",
    "    begin_end_train_test_timestamps = []\n",
    "    for j in range(0, len(begin_train_timestamp)):\n",
    "        begin_end_train_test_timestamp = (begin_train_timestamp[j], end_train_timestamp[j], begin_test_timestamp[j], end_test_timestamp[j])\n",
    "        begin_end_train_test_timestamps.append(begin_end_train_test_timestamp)\n",
    "    begin_end_train_test_timestamps\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_dict = {ts_names_train[ts].split('/')[len(ts_names_train[ts].split('/'))-1] : begin_end_train_test_timestamps}\n",
    "\n",
    "    dataset_split_dict.update(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9527fe72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ec2_cpu_utilization_24ae8d.csv': [('2014-02-14 14:30:00',\n",
       "   '2014-02-21 14:25:00',\n",
       "   '2014-02-21 14:30:00',\n",
       "   '2014-02-28 14:25:00')],\n",
       " 'ec2_cpu_utilization_53ea38.csv': [('2014-02-14 14:30:00',\n",
       "   '2014-02-21 14:25:00',\n",
       "   '2014-02-21 14:30:00',\n",
       "   '2014-02-28 14:25:00')],\n",
       " 'ec2_cpu_utilization_5f5533.csv': [('2014-02-14 14:27:00',\n",
       "   '2014-02-21 14:22:00',\n",
       "   '2014-02-21 14:27:00',\n",
       "   '2014-02-28 14:22:00')],\n",
       " 'ec2_cpu_utilization_77c1ca.csv': [('2014-04-02 14:25:00',\n",
       "   '2014-04-09 14:20:00',\n",
       "   '2014-04-09 14:25:00',\n",
       "   '2014-04-16 14:20:00')],\n",
       " 'ec2_cpu_utilization_825cc2.csv': [('2014-04-10 00:04:00',\n",
       "   '2014-04-17 00:09:00',\n",
       "   '2014-04-17 00:14:00',\n",
       "   '2014-04-24 00:09:00')],\n",
       " 'ec2_cpu_utilization_ac20cd.csv': [('2014-04-02 14:29:00',\n",
       "   '2014-04-09 14:34:00',\n",
       "   '2014-04-09 14:39:00',\n",
       "   '2014-04-16 14:49:00')],\n",
       " 'ec2_cpu_utilization_c6585a.csv': [('2014-04-02 14:29:00',\n",
       "   '2014-04-09 14:24:00',\n",
       "   '2014-04-09 14:29:00',\n",
       "   '2014-04-16 14:24:00')],\n",
       " 'ec2_cpu_utilization_fe7f93.csv': [('2014-02-14 14:27:00',\n",
       "   '2014-02-21 14:22:00',\n",
       "   '2014-02-21 14:27:00',\n",
       "   '2014-02-28 14:22:00')],\n",
       " 'ec2_disk_write_bytes_1ef3de.csv': [('2014-03-01 17:34:00',\n",
       "   '2014-03-09 22:34:00',\n",
       "   '2014-03-09 22:39:00',\n",
       "   '2014-03-16 22:34:00'),\n",
       "  ('2014-03-01 17:34:00',\n",
       "   '2014-03-16 22:34:00',\n",
       "   '2014-03-16 22:39:00',\n",
       "   '2014-03-18 03:39:00')],\n",
       " 'ec2_disk_write_bytes_c0d644.csv': [('2014-04-02 14:25:00',\n",
       "   '2014-04-09 14:20:00',\n",
       "   '2014-04-09 14:25:00',\n",
       "   '2014-04-16 14:20:00')],\n",
       " 'ec2_network_in_257a54.csv': [('2014-04-10 00:04:00',\n",
       "   '2014-04-17 00:09:00',\n",
       "   '2014-04-17 00:14:00',\n",
       "   '2014-04-24 00:09:00')],\n",
       " 'ec2_network_in_5abac7.csv': [('2014-03-01 17:36:00',\n",
       "   '2014-03-09 22:36:00',\n",
       "   '2014-03-09 22:41:00',\n",
       "   '2014-03-16 22:36:00'),\n",
       "  ('2014-03-01 17:36:00',\n",
       "   '2014-03-16 22:36:00',\n",
       "   '2014-03-16 22:41:00',\n",
       "   '2014-03-18 03:41:00')],\n",
       " 'elb_request_count_8c0756.csv': [('2014-04-10 00:04:00',\n",
       "   '2014-04-17 00:24:00',\n",
       "   '2014-04-17 00:29:00',\n",
       "   '2014-04-24 00:39:00')],\n",
       " 'grok_asg_anomaly.csv': [('2014-01-16 00:00:00',\n",
       "   '2014-01-24 00:25:00',\n",
       "   '2014-01-24 00:30:00',\n",
       "   '2014-01-31 00:25:00'),\n",
       "  ('2014-01-16 00:00:00',\n",
       "   '2014-01-31 00:25:00',\n",
       "   '2014-01-31 00:30:00',\n",
       "   '2014-02-01 01:00:00')],\n",
       " 'iio_us-east-1_i-a2eb1cd9_NetworkIn.csv': [('2013-10-09 16:25:00',\n",
       "   '2013-10-11 20:05:00',\n",
       "   '2013-10-11 20:10:00',\n",
       "   '2013-10-13 23:55:00')],\n",
       " 'rds_cpu_utilization_cc0c53.csv': [('2014-02-14 14:30:00',\n",
       "   '2014-02-21 14:25:00',\n",
       "   '2014-02-21 14:30:00',\n",
       "   '2014-02-28 14:30:00')],\n",
       " 'rds_cpu_utilization_e47b3b.csv': [('2014-04-10 00:02:00',\n",
       "   '2014-04-16 23:57:00',\n",
       "   '2014-04-17 00:02:00',\n",
       "   '2014-04-23 23:57:00')],\n",
       " 'real_1.csv': [(1.0, 710.0, 711.0, 1420.0)],\n",
       " 'real_10.csv': [(1.0, 719.0, 720.0, 1439.0)],\n",
       " 'real_11.csv': [(1, 719, 720, 1439)],\n",
       " 'real_12.csv': [(1, 719, 720, 1439)],\n",
       " 'real_13.csv': [(1, 719, 720, 1439)],\n",
       " 'real_14.csv': [(1, 719, 720, 1439)],\n",
       " 'real_15.csv': [(1, 719, 720, 1439)],\n",
       " 'real_16.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_17.csv': [(1, 712, 713, 1424)],\n",
       " 'real_18.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_19.csv': [(1, 712, 713, 1424)],\n",
       " 'real_2.csv': [(1, 719, 720, 1439)],\n",
       " 'real_20.csv': [(1.0, 711.0, 712.0, 1422.0)],\n",
       " 'real_21.csv': [(1, 710, 711, 1420)],\n",
       " 'real_22.csv': [(1, 710, 711, 1420)],\n",
       " 'real_23.csv': [(1.0, 710.0, 711.0, 1420.0)],\n",
       " 'real_24.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_25.csv': [(1, 717, 718, 1435)],\n",
       " 'real_26.csv': [(1, 717, 718, 1435)],\n",
       " 'real_27.csv': [(1, 713, 714, 1427)],\n",
       " 'real_28.csv': [(1, 720, 721, 1441)],\n",
       " 'real_29.csv': [(1, 720, 721, 1441)],\n",
       " 'real_3.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_30.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_31.csv': [(1, 713, 714, 1427)],\n",
       " 'real_32.csv': [(1.0, 713.0, 714.0, 1427.0)],\n",
       " 'real_33.csv': [(1, 719, 720, 1439)],\n",
       " 'real_34.csv': [(1, 713, 714, 1427)],\n",
       " 'real_35.csv': [(1, 713, 714, 1427)],\n",
       " 'real_36.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_37.csv': [(1, 717, 718, 1434)],\n",
       " 'real_38.csv': [(1, 713, 714, 1427)],\n",
       " 'real_39.csv': [(1, 713, 714, 1427)],\n",
       " 'real_4.csv': [(1, 711, 712, 1423)],\n",
       " 'real_40.csv': [(1, 713, 714, 1427)],\n",
       " 'real_41.csv': [(1, 717, 718, 1435)],\n",
       " 'real_42.csv': [(1, 720, 721, 1440)],\n",
       " 'real_43.csv': [(1.0, 720.0, 721.0, 1440.0)],\n",
       " 'real_44.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_45.csv': [(1, 720, 721, 1440)],\n",
       " 'real_46.csv': [(1, 720, 721, 1441)],\n",
       " 'real_47.csv': [(1, 713, 714, 1427)],\n",
       " 'real_48.csv': [(1, 719, 720, 1439)],\n",
       " 'real_49.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_5.csv': [(1, 719, 720, 1439)],\n",
       " 'real_50.csv': [(1, 719, 720, 1439)],\n",
       " 'real_51.csv': [(1, 713, 714, 1427)],\n",
       " 'real_52.csv': [(1, 716, 717, 1432)],\n",
       " 'real_53.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_54.csv': [(1.0, 370.0, 371.0, 741.0)],\n",
       " 'real_55.csv': [(1, 713, 714, 1427)],\n",
       " 'real_56.csv': [(1, 713, 714, 1427)],\n",
       " 'real_57.csv': [(1, 720, 721, 1441)],\n",
       " 'real_58.csv': [(1, 717, 718, 1435)],\n",
       " 'real_59.csv': [(1.0, 711.0, 712.0, 1423.0)],\n",
       " 'real_6.csv': [(1, 719, 720, 1439)],\n",
       " 'real_60.csv': [(1.0, 730.0, 731.0, 1461.0)],\n",
       " 'real_61.csv': [(1, 720, 721, 1441)],\n",
       " 'real_62.csv': [(1.0, 370.0, 371.0, 741.0)],\n",
       " 'real_63.csv': [(1.0, 719.0, 720.0, 1439.0)],\n",
       " 'real_64.csv': [(1, 720, 721, 1441)],\n",
       " 'real_65.csv': [(1, 712, 713, 1424)],\n",
       " 'real_66.csv': [(1, 712, 713, 1424)],\n",
       " 'real_67.csv': [(1, 711, 712, 1423)],\n",
       " 'real_7.csv': [(1, 711, 712, 1423)],\n",
       " 'real_8.csv': [(1, 710, 711, 1420)],\n",
       " 'real_9.csv': [(1.0, 730.0, 731.0, 1461.0)]}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_split_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4095d709",
   "metadata": {},
   "source": [
    "### Get Maximum of Splits over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed06c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_names = list(dataset_split_dict.keys())\n",
    "lengths_splits = []\n",
    "for ts_name in ts_names:\n",
    "    lengths_splits.append(len(dataset_split_dict[ts_name]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88053b05",
   "metadata": {},
   "source": [
    "### Duplicate Last Splits in Case Len < max(lenghts_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "796dda69",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ts_name in ts_names:\n",
    "    if(len(dataset_split_dict[ts_name])<np.max(lengths_splits)):\n",
    "        while(len(dataset_split_dict[ts_name])<np.max(lengths_splits)):\n",
    "            dataset_split_dict[ts_name].append(dataset_split_dict[ts_name][len(dataset_split_dict[ts_name])-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fbb26ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_splits = []\n",
    "for ts_name in ts_names:\n",
    "    lengths_splits.append(len(dataset_split_dict[ts_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2c007",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4dd3c1e0",
   "metadata": {},
   "source": [
    "# SRCNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d8cc8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAB\n",
    "path_merged = '../datasets/NAB_realAWSCloudwatch/'\n",
    "\n",
    "ts_names = []\n",
    "for i in os.listdir(path_merged):\n",
    "    ts_names.append(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "992466a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/17 [00:00<?, ?it/s]c:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 0/17 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8848/2365837918.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_train_start\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex_test_start\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmerged_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex_test_start\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mindex_test_end\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\oxifl\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4102\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4103\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast_scalar_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_float\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4104\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4106\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "generator = gen(window, step, num)\n",
    "model_update = 'fh'\n",
    "\n",
    "final_results = []\n",
    "final_delay = []\n",
    "final_f1_score = []\n",
    "final_precision = []\n",
    "final_recall = []\n",
    "final_TP = []\n",
    "final_FP = []\n",
    "final_TN = []\n",
    "final_FN = []\n",
    "final_seed = []\n",
    "\n",
    "df_results_final_seeds = pd.DataFrame(columns=['Seeds', 'Retraining_Technique', 'Delay', 'Results', \n",
    "                                        'F1_score', 'Precision', 'Recall', 'TP', 'FP', 'TN', 'FN'])\n",
    "df_results_final_seeds = df_results_final_seeds.reset_index(drop = True)\n",
    "\n",
    "\n",
    "for seed in range(0,5):\n",
    "    \n",
    "    #print('SEED: ', seed)    \n",
    "    final_seed.append(seed)\n",
    "\n",
    "\n",
    "\n",
    "    results_corrected_per_split = []\n",
    "\n",
    "    results_per_split = []\n",
    "\n",
    "    split_number_all = []\n",
    "    ts_all = []\n",
    "\n",
    "    results_all = []\n",
    "    total_fscore_all = []\n",
    "    pre_all = []\n",
    "    rec_all = []\n",
    "    TP_all = []\n",
    "    FP_all = []\n",
    "    TN_all = []\n",
    "    FN_all = []\n",
    "\n",
    "\n",
    "    labels_pred = []\n",
    "    labels_true = []\n",
    "\n",
    "\n",
    "    for i in range(0, np.max(lengths_splits)):\n",
    "        #print('Split Number: ', i)\n",
    "        split_number_all.append(i)\n",
    "        \n",
    "        \n",
    "        # GENERATE DATA FOR EACH TS\n",
    "\n",
    "        results = []\n",
    "        total_time = 0\n",
    "\n",
    "        for ts_name in tqdm.tqdm(ts_names):\n",
    "            #print('Time Series Name: ', ts_name)\n",
    "\n",
    "\n",
    "            merged_df = pd.read_csv(path_merged + ts_name)\n",
    "            \n",
    "            for j in range(0, len(merged_df)):\n",
    "                merged_df.timestamp[j] = float(j)\n",
    "\n",
    "            index_train_start = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][0]].index\n",
    "            index_train_end = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][1]].index\n",
    "\n",
    "            index_test_start = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][2]].index\n",
    "            index_test_end = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][3]].index\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            train = merged_df[index_train_start[0]:index_test_start[0]]\n",
    "            test = merged_df[index_test_start[0]:index_test_end[0]]\n",
    "\n",
    "\n",
    "            # GENERATE DATA\n",
    "\n",
    "\n",
    "\n",
    "            in_value = train['value']\n",
    "            #print(len(in_value))\n",
    "\n",
    "            if len(in_value) < window:\n",
    "                print(\"value's length < window size\", len(in_value), window)\n",
    "                continue\n",
    "\n",
    "            time_start = time.time()\n",
    "            train_data = generator.generate_train_data(in_value)\n",
    "            #print('TRAIN', len(train_data))\n",
    "            time_end = time.time()\n",
    "            total_time += time_end - time_start\n",
    "            results += train_data\n",
    "\n",
    "\n",
    "        train_data_path = os.getcwd() + '/' + data_train + '_' + str(window) + '_split_no_' + str(i) + '_train.json'\n",
    "\n",
    "        with open(train_data_path, 'w+') as f:\n",
    "            print(train_data_path)\n",
    "            json.dump(results, f)\n",
    "\n",
    "\n",
    "        # TRAIN\n",
    "\n",
    "        if auto:\n",
    "            data_train, window = auto(epoch)\n",
    "        else:\n",
    "            data_train, window = data_train, window\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        models = {\n",
    "            'sr_cnn': sr_cnn,\n",
    "        }\n",
    "        model = model\n",
    "        root_path = os.getcwd()\n",
    "\n",
    "        train_data_path = root_path + '/' + data_train + '_' + str(window) + '_split_no_' + str(i) + '_train.json'\n",
    "        model_path = root_path + '/' + save + '/'\n",
    "        if load:\n",
    "            load_path = root_path + '/' + load\n",
    "        else:\n",
    "            load_path = None\n",
    "\n",
    "        total_time = 0\n",
    "        time_start = time.time()\n",
    "        models[model](train_data_path, model_path, window, lr, epoch, batch_size, num_workers,\n",
    "                      load_path=load_path, model_update = model_update, update_split = i)\n",
    "        time_end = time.time()\n",
    "        total_time += time_end - time_start\n",
    "        # EVALUATE\n",
    "\n",
    "        if auto:\n",
    "            window, epoch = auto()\n",
    "        else:\n",
    "            window = window\n",
    "            epoch = epoch\n",
    "        delay = delay\n",
    "        model = model\n",
    "\n",
    "        path_snapshot = '../'\n",
    "\n",
    "        root = os.getcwd()\n",
    "        print(data, window, epoch)\n",
    "        models = {\n",
    "            'sr_cnn': sr_cnn_eval,\n",
    "        }\n",
    "\n",
    "        model_path = path_snapshot + '/' + model_path_snapshot + '/srcnn_retry' + '_' + model_update + '_' + str(i) + '_' + str(epoch) + '_' + str(window) + '.bin'\n",
    "\n",
    "        srcnn_model = Anomaly(window)\n",
    "        net = load_model(srcnn_model, model_path)\n",
    "\n",
    "        results_all = []\n",
    "\n",
    "\n",
    "        for ts_name in tqdm(ts_names):\n",
    "\n",
    "            ts_all.append(ts_name)\n",
    "\n",
    "            merged_df = pd.read_csv(path_merged + ts_name)\n",
    "            \n",
    "            for j in range(0, len(merged_df)):\n",
    "                merged_df.timestamp[j] = float(j)\n",
    "            \n",
    "\n",
    "            index_test_start = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][2]].index\n",
    "            index_test_end = merged_df[merged_df.timestamp == dataset_split_dict[ts_name][i][3]].index\n",
    "            \n",
    "\n",
    "\n",
    "            test = merged_df[index_test_start[0]:index_test_end[0]+1]\n",
    "\n",
    "            total_time, results, savedscore = get_score(test, ts_name, thres, missing_option)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            results_all.append(results)\n",
    "\n",
    "        #print('LEN results_all: ', len(results_all))\n",
    "\n",
    "\n",
    "        #total_fscore, pre, rec, TP, FP, TN, FN = evaluate_for_all_series(results_corrected, delay)\n",
    "\n",
    "        results_per_split.append(results_all)\n",
    "        # save predictions\n",
    "\n",
    "    ts_name = []\n",
    "    labels_true_final = []\n",
    "    labels_pred_final = []\n",
    "    timestamps_final = []\n",
    "\n",
    "    #ts_no = 44\n",
    "\n",
    "    for ts_no in range(0, len(ts_names)):\n",
    "        labels_true = []\n",
    "        labels_pred = []\n",
    "        timestamps = []\n",
    "\n",
    "        for i in range(0, len(results_per_split)):\n",
    "\n",
    "\n",
    "            for j in range(0, len(results_per_split[i][ts_no][0][1])):\n",
    "                labels_true.append(results_per_split[i][ts_no][0][1][j])\n",
    "\n",
    "            for k in range(0, len(results_per_split[i][ts_no][0][2])):    \n",
    "                labels_pred.append(results_per_split[i][ts_no][0][2][k])\n",
    "\n",
    "            for l in range(0, len(results_per_split[i][ts_no][0][0])):\n",
    "                timestamps.append(results_per_split[i][ts_no][0][0][l])\n",
    "\n",
    "        ts_name.append(results_per_split[i][ts_no][0][3])\n",
    "        labels_true_final.append(labels_true)\n",
    "        labels_pred_final.append(labels_pred)\n",
    "        timestamps_final.append(timestamps)\n",
    "    \n",
    "    list_results_final = []\n",
    "    for i in range(0, len(labels_true_final)):\n",
    "        list_results_intermediate = []\n",
    "        list_results_intermediate.append(timestamps_final[i])\n",
    "        list_results_intermediate.append(labels_true_final[i])\n",
    "        list_results_intermediate.append(labels_pred_final[i])\n",
    "        list_results_intermediate.append(ts_name[i])\n",
    "\n",
    "        list_results_final.append(list_results_intermediate)\n",
    "    list_results_intermediate\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    f1_per_delay = []\n",
    "    delay_list = []\n",
    "    precision_per_delay = []\n",
    "    recall_per_delay = []\n",
    "    TP_per_delay = []\n",
    "    FP_per_delay = []\n",
    "    TN_per_delay = []\n",
    "    FN_per_delay = []\n",
    "\n",
    "\n",
    "    for delay in range(0,8):\n",
    "        total_fscore, pre, rec, TP, FP, TN, FN = evaluate_for_all_series(list_results_final, delay)\n",
    "        delay_list.append(delay)\n",
    "        f1_per_delay.append(total_fscore)\n",
    "        precision_per_delay.append(pre)\n",
    "        recall_per_delay.append(rec)\n",
    "        TP_per_delay.append(TP)\n",
    "        FP_per_delay.append(FP)\n",
    "        TN_per_delay.append(TN)\n",
    "        FN_per_delay.append(FN)\n",
    "        \n",
    "    final_results.append(list_results_final)\n",
    "    final_delay.append(delay_list)\n",
    "    final_f1_score.append(f1_per_delay)\n",
    "    final_precision.append(precision_per_delay)\n",
    "    final_recall.append(recall_per_delay)\n",
    "    \n",
    "\n",
    "    final_TP.append(TP_per_delay)\n",
    "    final_FP.append(FP_per_delay)\n",
    "    final_TN.append(TN_per_delay)\n",
    "    final_FN.append(FN_per_delay)\n",
    "\n",
    "\n",
    "    # Create single row \n",
    "    \n",
    "    df_results_final = pd.DataFrame(columns=['Seeds', 'Retraining_Technique', 'Delay', 'Results', \n",
    "                                        'F1_score', 'Precision', 'Recall', 'TP', 'FP', 'TN', 'FN'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_results_final = pd.DataFrame(columns=['Seeds', 'Retraining_Technique', 'Delay', 'Results', \n",
    "                                        'F1_score', 'Precision', 'Recall', 'TP', 'FP', 'TN', 'FN'])\n",
    "    df_results_final['Seeds'] = [seed]\n",
    "\n",
    "\n",
    "    df_results_final['Retraining_Technique'] = [model_update]\n",
    "\n",
    "\n",
    "    df_results_final['Delay'] = [delay_list]\n",
    "    df_results_final['Results'] = [list_results_final]\n",
    "    df_results_final['F1_score'] = [f1_per_delay]\n",
    "    df_results_final['Precision'] = [precision_per_delay]\n",
    "    df_results_final['Recall'] = [recall_per_delay]\n",
    "    df_results_final['TP'] = [TP_per_delay]\n",
    "    df_results_final['FP'] = [FP_per_delay]\n",
    "    df_results_final['TN'] = [TN_per_delay]\n",
    "    df_results_final['FN'] = [FN_per_delay]\n",
    "\n",
    "    df_results_final = df_results_final.reset_index(drop = True)\n",
    "\n",
    "    df_results_final_seeds = df_results_final_seeds.append(df_results_final)\n",
    "\n",
    "\n",
    "df_results_final_seeds\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf1509",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_final_seeds = df_results_final_seeds.reset_index(drop = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
